tdm_sparse <- as(t(tdm_filtered), "sparseMatrix")
# Combine all matrices
genre_dummies <- model.matrix(~ genre - 1, data = game)
year_cont <- model.matrix(~ year_adj, data = game) # as.factor(
x_genre_sum_yr <- cbind(genre_dummies, year_cont, tdm_sparse)  # year_dummies,
x_genre_sum_yr <- as(x_genre_sum_yr, "sparseMatrix")
# str(combined_sparse_matrix)
# Run lasso on genre, summary words, and year
lasso_genre_sum_yr <- gamlr(x_genre_sum_yr, game$log_Players, standardize=TRUE, family = "gaussian", lambda.min.ratio=1e-3)
# Run lasso on genre, summary words, and year
lasso_genre_sum_yr <- gamlr(x_genre_sum_yr, game$log_activePlayers, standardize=TRUE, family = "gaussian", lambda.min.ratio=1e-3)
plot(lasso_genre_sum_yr)
# in-sample R2
1- lasso_genre_sum_yr$deviance[which.min(AICc(lasso_genre_sum_yr))]/lasso_genre_sum_yr$deviance[1]
# Check coefficients
lasso_genre_sum_yr_coef <- coef(lasso_genre_sum_yr, select=which.min(AICc(lasso_genre_sum_yr)))
sum(lasso_genre_sum_yr_coef!=0)
lasso_genre_sum_yr_coef <- as.data.frame(as.matrix(lasso_genre_sum_yr_coef))
colnames(lasso_genre_sum_yr_coef) <- c("Coefficient")
lasso_genre_sum_yr_coef$Feature <- rownames(lasso_genre_sum_yr_coef)
rownames(lasso_genre_sum_yr_coef) <- NULL
lasso_genre_sum_yr_coef_sig <- lasso_genre_sum_yr_coef %>% filter(Coefficient != 0)%>% arrange(desc(Coefficient))
lasso_genre_sum_yr_coef_sig[-1, ]
# Define filler words to be removed
filler_words <- c("the", "is", "a", "has", "have", "and", "of", "in", "to", "for", "with", "on", "that", "lets","as", "out", "by","from", "this", "be", "an", "v", "or", "so", "you", "are", "can", "will", "which", "t", "who", "where", "also", "his", "her", "their", "they", "up", "he", "she", "its", "it", "includes", "include","your", "you", "all","���������", "s", "any", "ll", "was", "but", "if", "there", "these")
# Function to clean and tokenize text, removing filler words
clean_and_tokenize <- function(text, filler_words) {
text_clean <- tolower(enc2utf8(text))
# text_clean <- tolower(gsub("[^\\x01-\\x7F]", "", text_clean))
text_clean <- str_replace_all(text_clean, "[[:punct:]]", " ")
words <- unlist(strsplit(text_clean, "\\s+"))
words <- words[!words %in% filler_words]
return(words)
}
# Apply the function to the Summary column
game$Summary_clean <- sapply(game$Summary, function(x) paste(clean_and_tokenize(x, filler_words),
collapse = " "))
# Create a corpus from the cleaned summaries
corpus <- Corpus(VectorSource(game$Summary_clean))
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, Inf)))
tdm_matrix <- as.matrix(tdm)
# Get word frequencies
word_freq <- sort(rowSums(tdm_matrix), decreasing = TRUE)
word_freq_df <- data.frame(word = names(word_freq), frequency = word_freq)
# Filter for significant words
sig_word_sum <- word_freq_df %>% filter(frequency > 5)
sig_word_sum <- sig_word_sum %>% arrange(word) %>% slice(-1:-19)
sig_word_sum <- as.character(sig_word_sum$word)
# Convert to sparse matrix
tdm_filtered <- tdm_matrix[sig_word_sum, ]
tdm_sparse <- as(t(tdm_filtered), "sparseMatrix")
# Filter the term-document matrix to keep only significant words
tdm_filtered <- tdm_matrix[sig_word_sum, ]
tdm_sparse <- as(t(tdm_filtered), "sparseMatrix")
# Combine all matrices
genre_dummies <- model.matrix(~ genre - 1, data = game)
year_cont <- model.matrix(~ year_adj, data = game) # as.factor(
x_genre_sum_yr <- cbind(genre_dummies, year_cont, tdm_sparse)  # year_dummies,
x_genre_sum_yr <- as(x_genre_sum_yr, "sparseMatrix")
# str(combined_sparse_matrix)
# Run lasso on genre, summary words, and year
lasso_genre_sum_yr <- gamlr(x_genre_sum_yr, game$log_activePlayers, standardize=TRUE, family = "gaussian", lambda.min.ratio=1e-3)
plot(lasso_genre_sum_yr)
# in-sample R2
1- lasso_genre_sum_yr$deviance[which.min(AICc(lasso_genre_sum_yr))]/lasso_genre_sum_yr$deviance[1]
# Check coefficients
lasso_genre_sum_yr_coef <- coef(lasso_genre_sum_yr, select=which.min(AICc(lasso_genre_sum_yr)))
sum(lasso_genre_sum_yr_coef!=0)
lasso_genre_sum_yr_coef <- as.data.frame(as.matrix(lasso_genre_sum_yr_coef))
colnames(lasso_genre_sum_yr_coef) <- c("Coefficient")
lasso_genre_sum_yr_coef$Feature <- rownames(lasso_genre_sum_yr_coef)
rownames(lasso_genre_sum_yr_coef) <- NULL
lasso_genre_sum_yr_coef_sig <- lasso_genre_sum_yr_coef %>% filter(Coefficient != 0)%>% arrange(desc(Coefficient))
lasso_genre_sum_yr_coef_sig[-1, ]
# Run CV lasso on genre, summary words, and year
set.seed(1234)
y_log <- game$log_Players
lasso_genre_sum_yr.cv <- cv.gamlr(x_genre_sum_yr, game$log_activePlayers, lambda.min.ratio = 1e-3, family = "gaussian", verb = TRUE)
# Plot the cross-validation results
plot(lasso_genre_sum_yr.cv)
abline(v=log(lasso_genre_sum_yr$lambda[which.min(AICc(lasso_genre_sum_yr))]))
# Display the coefficients of the Lasso model at the optimal lambda
coef_min <- coef(lasso_genre_sum_yr.cv, s = "min")
coef_1se <- coef(lasso_genre_sum_yr.cv, s = "1se")
Betas <- drop(coef_min) # AICc default selection: Betas <- drop(coef(lasso_genre_sum_yr))
length(coef_min)# intercept + 17 genres + 19 year + 1203 words
sum(coef_min!=0) # 1203 predictive words
# Choose 10 most positive review words in this model
o<-order(coef_min,decreasing=TRUE)  # [38:1240]
kable(Betas[o[1:10]])
o<-order(coef_min,decreasing=FALSE)  # [38:1240]
kable(Betas[o[1:15]])
# Genre Popularity Over the Years
# Plot genre popularity over the years
ggplot(game, aes(x = year, fill = genre)) +
geom_bar(position = "dodge") +
labs(title = "Genre Popularity Over the Years",
x = "Year",
y = "Count",
fill = "Genre") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
# Plot the average number of plays per genre over the years
ggplot(game, aes(x = year, y = Plays, fill = genre)) +
geom_bar(stat = "summary", fun = "mean", position = "dodge") +
labs(title = "Average Number of Plays per Genre Over the Years",
x = "Year",
y = "Average Plays",
fill = "Genre") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
# Prepare data for regression
# game$log_Plays <- log(game$Plays + 1) # Add 1 to avoid log(0)
# Linear Regression
lm_model <- lm(log_activePlayers ~ genre + year + publisher + atvi_indi, data = game)
summary(lm_model)
# Lasso Regression
x <- model.matrix(log_activePlayers ~ genre + year + publisher + atvi_indi + Rating + Number.of.Reviews, data = game)[, -1]
lasso_model <- cv.glmnet(x, game$log_Players, alpha = 1)
# Lasso Regression
x <- model.matrix(log_activePlayers ~ genre + year + publisher + atvi_indi + Rating + Number.of.Reviews, data = game)[, -1]
lasso_model <- cv.glmnet(x, game$log_activePlayers, alpha = 1)
plot(lasso_model)
# Coefficients of the Lasso model at the optimal lambda
lasso_coefs <- coef(lasso_model, s = "lambda.min")
print(lasso_coefs)
games <- read.csv('games.csv')
setwd("~/GitHub/bigdata_g12/FinalProject")
games <- read.csv('games.csv')
setwd("~/GitHub/bigdata_g12/FinalProject")
game <- read.csv("game.csv", fill = TRUE)
library(igraph)
# split Team into list of devs
games$Team <- gsub("\\['|'\\]|'", "", games$Team)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(tidyr)
library(stringr)
library(corrplot)
library(lubridate)
library(Matrix)  # For sparse matrices
library(gamlr)  # For regression models
library(parallel)  # For parallel computing
library(ggplot2)  # For plotting
library(knitr)
library(textir)
library(maptpx)
library(slam)
library(wordcloud)
library(gamlr)
library(tm)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(lubridate)
library(caret)
library(glmnet)
library(rpart)
library(rpart.plot)
library(randomForest)
linear2 <- lm(activePlayers ~ year * genre + Rating + Number.of.Reviews, data = game)
summary(linear2)
linear2 <- lm(activePlayers ~ year * genre + Rating + Number.of.Reviews, data = game)
summary(linear2)
linear3 <- lm(log_activePlayers ~ year * genre + Rating + Number.of.Reviews, data = game)
linear2 <- lm(activePlayers ~ year * genre + Rating + Number.of.Reviews, data = game)
summary(linear2)
linear3 <- lm(log(activePlayers) ~ year * genre + Rating + Number.of.Reviews, data = game)
summary(linear3)
linear2 <- lm(log(activePlayers) ~ year * genre + Rating + Number.of.Reviews, data = game)
summary(linear2)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(tidyr)
library(stringr)
library(corrplot)
library(lubridate)
library(Matrix)  # For sparse matrices
library(gamlr)  # For regression models
library(parallel)  # For parallel computing
library(ggplot2)  # For plotting
library(knitr)
library(textir)
library(maptpx)
library(slam)
library(wordcloud)
library(gamlr)
library(tm)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(lubridate)
library(caret)
library(glmnet)
library(rpart)
library(rpart.plot)
library(randomForest)
col_purple_orange <- colorRampPalette(c("orange","purple"))
# Plot the correlation matrix with correlation coefficients
numeric_vars <- game %>% select(activePlayers, allPlayers, Number.of.Reviews, Rating, Backlogs, Wishlist, year)
cor_matrix <- cor(numeric_vars)
corrplot(cor_matrix, method = "circle", col = col_purple_orange(200),
type = "upper", order = "hclust", tl.col = "black", tl.srt = 45,
addCoef.col = "black", cl.cex = 0.8, number.cex = 0.8)
title("Correlation Matrix of Numeric Variables", line = 2.5, cex.main = 1.2)
pairs(numeric_vars, col = "orange", main = "Pairs Plot of Numeric Variables")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(tidyr)
library(stringr)
library(corrplot)
library(lubridate)
library(Matrix)  # For sparse matrices
library(gamlr)  # For regression models
library(parallel)  # For parallel computing
library(ggplot2)  # For plotting
library(knitr)
library(textir)
library(maptpx)
library(slam)
library(wordcloud)
library(gamlr)
library(tm)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(lubridate)
library(caret)
library(glmnet)
library(rpart)
library(rpart.plot)
library(randomForest)
# Load necessary libraries
library(Matrix)  # For sparse matrices
library(gamlr)   # For regression models
library(parallel)  # For parallel computing
library(ggplot2)  # For plotting
library(p.adjust)  # For adjusting p-values
game$year_adj <- scale(game$year_adj)
View(game)
View(game)
# Data Cleaning
game <- read.csv("game.csv", fill = TRUE)
game$year_adj <- game$year - 2000
game$log_activePlayers <- log(game$activePlayers+1)
game$log_allPlayers <- log(game$allPlayers+1)
game <- game %>% mutate(activePlayers_dummy = ifelse(allPlayers > median(game$activePlayers), 1, 0)) #
game <- game %>% mutate(allPlayers_dummy = ifelse(allPlayers > median(game$allPlayers), 1, 0))
game <- game %>% mutate(Rating_dummy = ifelse(Rating > 4, 1, 0))
summary(game)
library(Matrix)  # For sparse matrices
library(gamlr)   # For regression models
library(parallel)  # For parallel computing
library(ggplot2)  # For plotting
game$Number_of_Reviews <- scale(game$Number.of.Reviews)
game$publisher <- as.factor(game$publisher)
game$genre <- as.factor(game$genre)
game$Rating <- as.factor(game$Rating)
game$Wishlist <- as.factor(game$Wishlist)
consoles_list <- strsplit(game$console, split = " ")
unique_consoles <- unique(unlist(consoles_list))
console_matrix <- matrix(0, nrow = nrow(game), ncol = length(unique_consoles), dimnames = list(NULL, unique_consoles))
for (i in seq_along(consoles_list)) {
console_matrix[i, consoles_list[[i]]] <- 1
}
console_sparse <- Matrix(console_matrix, sparse = TRUE)
console_df <- as.data.frame(as.matrix(console_sparse))
colnames(console_df) <- unique_consoles
# Combine all predictors into one data frame
game_console <- cbind(game[, c("log_activePlayers", "year_adj", "Number_of_Reviews", "publisher", "genre", "Rating", "Wishlist")], console_df)
# Fit the linear model
fit <- lm(log_activePlayers ~ ., data=game_console)
# Get summary of the model
summary_fit <- summary(fit)
mrgpvals <- summary_fit$coefficients[, 4]
# Load FDR script and apply FDR correction
source("fdr.R")  # Assuming fdr.R defines a function fdr_cut() correctly
cutoff <- fdr_cut(mrgpvals, 0.1, TRUE)  # Using a threshold of 0.1 for demonstration
# Identify significant coefficients based on the FDR cutoff
significant_indices <- mrgpvals < cutoff
significant_coefficients <- summary_fit$coefficients[significant_indices, ]
significant_predictors <- names(significant_indices)[significant_indices]
# Output the table of significant predictors
print("Significant Predictors:")
print(significant_predictors)
lasso3 <- gamlr(x_genre, y, standardize=TRUE, lambda.min.ratio=1e-3)
plot(lasso3)
best_lambda3 <- lasso3$lambda[which.min(AICc(lasso3))]
best_lambda3
log(best_lambda3)
lasso3coef <- coef(lasso3, select=which.min(AICc(lasso3)))
lasso3coef <- lasso3coef[-1, ]
lasso3 <- gamlr(x_genre, y, standardize=TRUE, lambda.min.ratio=1e-3)
plot(lasso3)
best_lambda3 <- lasso3$lambda[which.min(AICc(lasso3))]
best_lambda3
log(best_lambda3)
lasso3coef <- coef(lasso3, select=which.min(AICc(lasso3)))
lasso3coef <- lasso3coef[-1, ]
lasso3coef
par(mfrow=c(1,2))
plot(lasso3$gamlr)
plot(lasso3$gamlr)
lasso3$gamlr
plot(lasso1$gamlr)
lasso1$gamlr
library(glmnet)
x_genre <- model.matrix(log_activePlayers ~ year_adj + genre + Rating + Number.of.Reviews, data = game)[,-1]  #  publisher +
x_genre <- as(x_genre, "sparseMatrix")
y <- game$activePlayers
lasso1 <- cv.glmnet(x_genre, y, family="gaussian")
plot(lasso1)
coef(lasso1, s = "lambda.min")
# lasso1_ <- gamlr(cbind(game$year_adj + game$genre + game$Rating + game$Number.of.Reviews), game$log_activePlayers, standardize=TRUE, family = "gaussian", lambda.min.ratio=1e-3)
par(mfrow=c(1,2))
plot(lasso1$gamlr)
sum(coef(lasso1)!=0) # 1se
sum(coef(lasso1, s="min")!=0) # min
coef(lasso1$gamlr)!=0
lasso1
lasso1$gamlr
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(tidyr)
library(stringr)
library(corrplot)
library(lubridate)
library(Matrix)  # For sparse matrices
library(gamlr)  # For regression models
library(parallel)  # For parallel computing
library(ggplot2)  # For plotting
library(knitr)
library(textir)
library(maptpx)
library(slam)
library(wordcloud)
library(gamlr)
library(tm)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(lubridate)
library(caret)
library(glmnet)
library(rpart)
library(rpart.plot)
library(randomForest)
set.seed(1234)
# Function to split data and calculate OOS R² for LASSO
calculate_oos_r2 <- function(x, y, train_fraction = 0.7, n_splits = 20) {
oos_r2_aicc <- numeric(n_splits)
oos_r2_cv_min <- numeric(n_splits)
for (i in 1:n_splits) {
# Split data into training and testing sets
train_indices <- sample(1:nrow(x), size = floor(train_fraction * nrow(x)))
test_indices <- setdiff(1:nrow(x), train_indices)
x_train <- x[train_indices, ]
y_train <- y[train_indices]
x_test <- x[test_indices, ]
y_test <- y[test_indices]
# Fit LASSO model with glmnet
lasso_model <- glmnet(x_train, y_train, alpha = 1)
lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1, lambda.min.ratio = 1e-3, nfolds = 5)
# Select lambda using AICc
gamlr_model <- as.gamlr(lasso_model)
aicc_values <- AICc(gamlr_model)
lambda_aicc <- gamlr_model$lambda[which.min(aicc_values)]
# Predict on test set using the lambda with minimum AICc
y_pred_aicc <- predict(lasso_model, s = lambda_aicc, newx = x_test)
# Calculate OOS R² for AICc
rss_aicc <- sum((y_test - y_pred_aicc)^2)
tss_aicc <- sum((y_test - mean(y_test))^2)
oos_r2_aicc[i] <- 1 - rss_aicc/tss_aicc
# Predict on test set using the lambda with minimum cross-validated error
y_pred_cv_min <- predict(lasso_cv, newx = x_test, s = "lambda.min")
# Calculate OOS R² for CV.min
rss_cv_min <- sum((y_test - y_pred_cv_min)^2)
tss_cv_min <- sum((y_test - mean(y_test))^2)
oos_r2_cv_min[i] <- 1 - rss_cv_min/tss_cv_min
}
return(list(oos_r2_aicc = oos_r2_aicc, oos_r2_cv_min = oos_r2_cv_min))
}
# Apply function to your data
x <- as.matrix(x)  # Ensure x is a matrix
y <- game$log_activePlayers
oos_r2_results <- calculate_oos_r2(x, y)
set.seed(1234)
# Function to split data and calculate OOS R² for LASSO using AICc and CV.min
calculate_oos_r2 <- function(x, y, train_fraction = 0.7, n_splits = 20) {
oos_r2_aicc <- numeric(n_splits)
oos_r2_cv_min <- numeric(n_splits)
for (i in 1:n_splits) {
# Split data into training and testing sets
train_indices <- sample(1:nrow(x), size = floor(train_fraction * nrow(x)))
test_indices <- setdiff(1:nrow(x), train_indices)
x_train <- x[train_indices, ]
y_train <- y[train_indices]
x_test <- x[test_indices, ]
y_test <- y[test_indices]
# Fit LASSO model with gamlr
lasso_model <- gamlr(x_train, y_train, lambda.min.ratio = 1e-3, family = "gaussian")
lasso_cv <- cv.gamlr(x_train, y_train, lambda.min.ratio = 1e-3, family = "gaussian")
# Select lambda using AICc
aicc_values <- AICc(lasso_model)
lambda_aicc <- lasso_model$lambda[which.min(aicc_values)]
# Predict on test set using the lambda with minimum AICc
y_pred_aicc <- predict(lasso_model, newdata = x_test, lambda = lambda_aicc)
# Calculate OOS R² for AICc
rss_aicc <- sum((y_test - y_pred_aicc)^2)
tss_aicc <- sum((y_test - mean(y_test))^2)
oos_r2_aicc[i] <- 1 - rss_aicc/tss_aicc
# Predict on test set using the lambda with minimum cross-validated error
y_pred_cv_min <- predict(lasso_cv, newdata = x_test, select = "min")
# Calculate OOS R² for CV.min
rss_cv_min <- sum((y_test - y_pred_cv_min)^2)
tss_cv_min <- sum((y_test - mean(y_test))^2)
oos_r2_cv_min[i] <- 1 - rss_cv_min/tss_cv_min
}
return(list(oos_r2_aicc = oos_r2_aicc, oos_r2_cv_min = oos_r2_cv_min))
}
# Apply function to your data
x <- as.matrix(x)  # Ensure x is a matrix
y <- game$log_activePlayers
oos_r2_results <- calculate_oos_r2(x, y)
# Combine OOS R² values into a data frame
model_names <- rep(c("LASSO_AICc", "LASSO_CVmin"), each = 20)
oos_r2_values <- c(oos_r2_results$oos_r2_aicc, oos_r2_results$oos_r2_cv_min)
results_df <- data.frame(model = model_names, OOS_R2 = oos_r2_values)
# Plot using ggplot2
ggplot(results_df, aes(x = model, y = OOS_R2)) +
geom_boxplot(fill = "blue", color = "black") +
theme_minimal() +
labs(title = "OOS R² Comparison for LASSO (AICc vs CV.min)", x = "Model", y = "OOS R²") +
theme(plot.title = element_text(hjust = 0.5))
install.packages("rpart")
install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
set.seed(1234)
# Fit the unpruned decision tree model
tree_model <- rpart(y ~ ., data = data.frame(y = y, x = as.data.frame(x)), control = rpart.control(cp = 0))
# Plot the unpruned tree
rpart.plot(tree_model, main = "Unpruned Decision Tree")
# Function to split data and calculate OOS R² for the unpruned tree
calculate_oos_r2_tree <- function(x, y, train_fraction = 0.7, n_splits = 20) {
oos_r2_values <- numeric(n_splits)
for (i in 1:n_splits) {
# Split data into training and testing sets
train_indices <- sample(1:nrow(x), size = floor(train_fraction * nrow(x)))
test_indices <- setdiff(1:nrow(x), train_indices)
x_train <- x[train_indices, ]
y_train <- y[train_indices]
x_test <- x[test_indices, ]
y_test <- y[test_indices]
# Fit the unpruned decision tree model
tree_model <- rpart(y_train ~ ., data = data.frame(y_train, x_train), control = rpart.control(cp = 0))
# Predict on the test set
y_pred <- predict(tree_model, newdata = data.frame(x_test))
# Calculate OOS R²
rss <- sum((y_test - y_pred)^2)
tss <- sum((y_test - mean(y_test))^2)
oos_r2 <- 1 - rss/tss
oos_r2_values[i] <- oos_r2
}
return(oos_r2_values)
}
# Apply function to your data
oos_r2_tree <- calculate_oos_r2_tree(x, y)
install.packages("rpart.plot")
install.packages("rpart")
install.packages("rpart")
install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
set.seed(1234)
# Fit the unpruned decision tree model
tree_model <- rpart(y ~ ., data = data.frame(y = y, x = as.data.frame(x)), control = rpart.control(cp = 0))
# Plot the unpruned tree
rpart.plot(tree_model, main = "Unpruned Decision Tree")
# Function to split data and calculate OOS R² for the unpruned tree
calculate_oos_r2_tree <- function(x, y, train_fraction = 0.7, n_splits = 20) {
oos_r2_values <- numeric(n_splits)
for (i in 1:n_splits) {
# Split data into training and testing sets
train_indices <- sample(1:nrow(x), size = floor(train_fraction * nrow(x)))
test_indices <- setdiff(1:nrow(x), train_indices)
x_train <- x[train_indices, ]
y_train <- y[train_indices]
x_test <- x[test_indices, ]
y_test <- y[test_indices]
# Fit the unpruned decision tree model
tree_model <- rpart(y_train ~ ., data = data.frame(y_train, x_train), control = rpart.control(cp = 0))
# Predict on the test set
y_pred <- predict(tree_model, newdata = data.frame(x_test))
# Calculate OOS R²
rss <- sum((y_test - y_pred)^2)
tss <- sum((y_test - mean(y_test))^2)
oos_r2 <- 1 - rss/tss
oos_r2_values[i] <- oos_r2
}
return(oos_r2_values)
}
# Apply function to your data
oos_r2_tree <- calculate_oos_r2_tree(x, y)
