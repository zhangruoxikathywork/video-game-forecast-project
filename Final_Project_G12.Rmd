---
title: "Final_Project_G12"
author: 'G12: Yufei Liu, Kathy Zhang, Liujun Hua'
date: "2024-05-12"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(tidyr)
library(stringr)
library(corrplot)
library(lubridate)
library(Matrix)  # For sparse matrices
library(gamlr)  # For regression models
library(parallel)  # For parallel computing
library(ggplot2)  # For plotting
library(knitr)
library(textir)
library(maptpx)
library(slam)
library(wordcloud)
library(gamlr)
library(tm)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(lubridate)
library(caret)
library(glmnet)
library(rpart)
library(rpart.plot)
library(randomForest)
```

```{r}
setwd("~/GitHub/bigdata_g12/FinalProject")

# Load the datasets
popular_vg <- read.csv("popular_vg_1980-2023.csv", stringsAsFactors = FALSE)
vgchartz_2024 <- read.csv("vgchartz-2024.csv", stringsAsFactors = FALSE)
names(vgchartz_2024)[names(vgchartz_2024) == "title"] <- "Title"

popular_vg$Release.Date <- as.Date(popular_vg$Release.Date, format="%Y-%m-%d")
popular_vg$year <- year(popular_vg$Release.Date)
popular_vg <- popular_vg[order(popular_vg$year), ]

popular_vg <- popular_vg %>%
  group_by(Title) %>%
  summarise(
    Release.Date = first(Release.Date),
    year = first(year),
    Team = first(Team),
    Rating = first(Rating),
    Times.Listed = first(Times.Listed),
    Number.of.Reviews = first(Number.of.Reviews),
    Genres = first(Genres),
    Summary = first(Summary),
    Reviews = paste(unique(Reviews), collapse = " "),
    Plays = first(Plays),
    Playing = first(Playing),
    Backlogs = first(Backlogs),
    Wishlist = first(Wishlist),
    .groups = 'drop'
  )

vgchartz_2024 <- vgchartz_2024 %>%
  group_by(Title) %>%
  summarise(
    console = paste(unique(console), collapse = " "),
    publisher = first(publisher),
    developer = first(developer),
    genre = first(genre),
    total_sales = sum(total_sales),  # Corrected typo here
    .groups = 'drop'
  )

# Perform the inner join
game <- inner_join(vgchartz_2024[, c("Title", "console", "publisher", "developer", "genre", "total_sales")], popular_vg[, c("Title","Release.Date", "Genres", "Rating", "Times.Listed", "year", "Number.of.Reviews", "Summary", "Reviews", "Plays", "Playing", "Backlogs", "Wishlist")], by = "Title")
game <- game %>% distinct()

# Convert 'Plays' and 'Number.of.Reviews' from K format to numeric
game$Plays <- as.numeric(str_replace(game$Plays, "K", "")) * 1000
game$Playing <- as.numeric(str_replace(game$Playing, "K", "")) * 1000
game$Backlogs <- as.numeric(str_replace(game$Backlogs, "K", "")) * 1000
game$Wishlist <- as.numeric(str_replace(game$Wishlist, "K", "")) * 1000
game$Number.of.Reviews <- as.numeric(str_replace(game$Number.of.Reviews, "K", "")) * 1000

game <- game %>% filter(year >= 2005)
# Remove rows with NAs only in the specified columns
columns_to_clean <- c("Rating", "Summary", "Reviews", "year")
game <- game %>%
  filter(!if_any(all_of(columns_to_clean), is.na))

game <- game %>%
  mutate(atvi_indi = ifelse(publisher %in% c("Activision", "Blizzard Entertainment") | developer == "Blizzard Entertainment", 1, 0))
game$activePlayers <- game$Plays + game$Playing
game$allPlayers <- game$Plays + game$Playing + game$Backlogs  

write.csv(game, "game.csv", row.names = FALSE)
head(game)
colnames(game)
```

```{r}
# Data Cleaning
game <- read.csv("game.csv", fill = TRUE)
game$year_adj <- game$year - 2000
game$log_activePlayers <- log(game$activePlayers+1)
game$log_allPlayers <- log(game$allPlayers+1)
game <- game %>% mutate(activePlayers_dummy = ifelse(allPlayers > median(game$activePlayers), 1, 0)) # 
# game <- game %>% mutate(allPlayers_dummy = ifelse(allPlayers > median(game$allPlayers), 1, 0))
# game <- game %>% mutate(Rating_dummy = ifelse(Rating > 4, 1, 0))
summary(game)
```


```{r}
## Clean Summary words
# Define filler words to be removed
filler_words <- c("the", "is", "a", "has", "have", "and", "of", "in", "to", "for", "with", "on", "that", "lets","as", "out", "by","from", "this", "be", "an", "v", "or", "so", "you", "are", "can", "will", "which", "t", "who", "where", "also", "his", "her", "their", "they", "up", "he", "she", "its", "it", "includes", "include","your", "you", "all","���������", "s", "any", "ll", "was", "but", "if", "there", "these")

# Function to clean and tokenize text, removing filler words
clean_and_tokenize <- function(text, filler_words) {
  text_clean <- tolower(enc2utf8(text))
  # text_clean <- tolower(gsub("[^\\x01-\\x7F]", "", text_clean))
  text_clean <- str_replace_all(text_clean, "[[:punct:]]", " ")
  words <- unlist(strsplit(text_clean, "\\s+"))
  words <- words[!words %in% filler_words]
  return(words)
}

# Apply the function to the Summary column
game$Summary_clean <- sapply(game$Summary, function(x) paste(clean_and_tokenize(x, filler_words), collapse = " "))

# Create a corpus from the cleaned summaries
corpus <- Corpus(VectorSource(game$Summary_clean))
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, Inf)))
tdm_matrix <- as.matrix(tdm)


# Get word frequencies
word_freq <- sort(rowSums(tdm_matrix), decreasing = TRUE)
word_freq_df <- data.frame(word = names(word_freq), frequency = word_freq)

# Filter for significant words
sig_word_sum <- word_freq_df %>% filter(frequency > 5)
sig_word_sum <- sig_word_sum %>% arrange(word) %>% slice(-1:-19)
sig_word_sum <- as.character(sig_word_sum$word)

# Convert to sparse matrix
tdm_filtered <- tdm_matrix[sig_word_sum, ]
tdm_sparse <- as(t(tdm_filtered), "sparseMatrix")


# Filter the term-document matrix to keep only significant words
tdm_filtered <- tdm_matrix[sig_word_sum, ]
tdm_sparse <- as(t(tdm_filtered), "sparseMatrix")

```

```{r}
## Yufei's IP
game$series <- sapply(str_split(game$Title, "\\s+"), function(words) {
  paste(words[1:min(length(words), 2)], collapse = " ")
})

# 计算每个系列的出现次数
series_counts <- game %>%
  group_by(series, publisher) %>%
  summarise(Count = n(), .groups = 'drop')

# 将 series_counts 加入到原数据框中
game <- game %>%
  left_join(series_counts, by = c("series", "publisher"))

# 标记大IP和小IP
game <- game %>%
  mutate(IP_Type = case_when(
    Count >= 6 ~ "Big IP",
    Count >= 4 ~ "Medium IP",
    Count >= 2 ~ "Small IP",
    TRUE ~ "Not IP"
  ))

game <- game %>%
  mutate(activePlayersGroup = cut(activePlayers, breaks = c(-Inf, 120000, 170000, Inf), labels = c("Low", "Medium", "High")))

ip_data <- game %>%
  filter(IP_Type %in% c("Big IP", "Small IP","Medium IP"))

ip_data$activePlayers <- as.numeric(ip_data$activePlayers)
table(ip_data$IP_Type)

ip_data$activePlayers <- as.numeric(ip_data$activePlayers)

summary(ip_data$activePlayers)

ip_avg_active_players <- ip_data %>%
  group_by(IP_Type) %>%
  summarise(
    avg_activePlayers = mean(activePlayers, na.rm = TRUE),
    max_activePlayers = max(activePlayers, na.rm = TRUE),
    min_activePlayers = min(activePlayers, na.rm = TRUE),
    count = n()
  )

library(glmnet)
library(caret)
game <- game %>%
  filter(!is.na(Rating) & !is.na(genre) & !is.na(IP_Type))
game$IP_Type <- factor(game$IP_Type, levels = c("Big IP", "Medium IP", "Small IP", "Not IP"))
```



```{r}
## Processing Console

# Split the console column by spaces (or your actual delimiter)
consoles_list <- strsplit(game$console, split = " ")

# Flatten the list into a single vector and get unique console types
unique_consoles <- unique(unlist(consoles_list))

# Create a matrix where each row corresponds to a game and each column to a console, filled initially with 0s
console_matrix <- matrix(0, nrow = nrow(game), ncol = length(unique_consoles), dimnames = list(NULL, unique_consoles))
for (i in seq_along(consoles_list)) {
  console_matrix[i, consoles_list[[i]]] <- 1
}
console_sparse <- Matrix(console_matrix, sparse = TRUE)
console_df <- as.data.frame(as.matrix(console_sparse))
colnames(console_df) <- unique_consoles
```


```{r}
set.seed(1234)

## Combine x 
genre_dummies <- model.matrix(~ genre - 1, data = game)
year_cont <- model.matrix(~ year_adj, data = game)
publisher_dummies <- model.matrix(~ publisher - 1, data = game)
game$Rating <- as.numeric(game$Rating)
rating_cont <- model.matrix(~ Rating, data = game)
ip_dummies <- model.matrix(~ IP_Type - 1, data = game)
x <- cbind(genre_dummies, year_cont, publisher_dummies, rating_cont, ip_dummies, tdm_sparse, console_sparse)
x <- as(x, "sparseMatrix")

# x <- cbind(game[, c("year_adj", "publisher", "genre", "IP_Type", "Rating")], console_df)
# x <- as(x, "sparseMatrix")  # or do not make sparse matrix
y <- game$log_activePlayers
```



## Explore potential Y variables
```{r}
# Plot the distribution of total_sales and plays
ggplot(game, aes(x = total_sales)) + 
  geom_histogram(bins = 30, fill = "pink", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of Total Sales (2005-2023)", x = "Total Sales", y = "Frequency") +
  theme(plot.title = element_text(hjust = 0.5),
        text = element_text(size = 12),
        axis.title = element_text(size = 12, face = "bold"))

ggplot(game, aes(x = total_sales)) + 
  geom_histogram(bins = 30, fill = "pink", color = "black") +
  theme_minimal() +
  scale_x_log10() +
  labs(title = "Distribution of Total Sales (Log Scale) (2005-2023)", x = "Total Sales (Log Scale)", y = "Frequency") +
  theme(plot.title = element_text(hjust = 0.5),
        text = element_text(size = 12),
        axis.title = element_text(size = 12, face = "bold"))


# Reshaping the data to long format
game_long <- game %>%
  # select(allPlayers, activePlayers) %>%  # Select the necessary columns
  pivot_longer(
    cols = c(allPlayers, activePlayers),
    names_to = "PlayerType",
    values_to = "Players"
  ) 

ggplot(game_long, aes(x = Players, fill = PlayerType)) +
  geom_histogram(bins = 30, alpha = 0.6, position = "identity", color = "black") +
  scale_fill_manual(values = c("orange", "purple")) +
  labs(title = "Distribution of All Players vs. Active Players (2005-2023)",
       x = "Number of Players",
       y = "Frequency") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        text = element_text(size = 12),
        axis.title = element_text(size = 12, face = "bold"))


ggplot(game_long, aes(x = Players, fill = PlayerType)) +
  geom_histogram(bins = 30, alpha = 0.6, position = "identity", color = "black") +
  scale_x_log10() + 
  scale_fill_manual(values = c("orange", "purple")) +
  labs(title = "Distribution of All Players vs. Active Players (Log Scale) (2005-2023)",
       x = "Number of Players (Log Scale)",
       y = "Frequency") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        text = element_text(size = 12),
        axis.title = element_text(size = 12, face = "bold"))
```

## EDA
```{r}
col_purple_orange <- colorRampPalette(c("orange","purple"))
# Plot the correlation matrix with correlation coefficients
numeric_vars <- game[, c("activePlayers", "allPlayers", "Number.of.Reviews", "Rating", "Backlogs", "Wishlist", "year")]
cor_matrix <- cor(numeric_vars)
corrplot(cor_matrix, method = "circle", col = col_purple_orange(200),
         type = "upper", order = "hclust", tl.col = "black", tl.srt = 45,
         addCoef.col = "black", cl.cex = 0.8, number.cex = 0.8)
title("Correlation Matrix of Numeric Variables", line = 2.5, cex.main = 1.2)
pairs(numeric_vars, col = "orange", main = "Pairs Plot of Numeric Variables")

cor_matrix
```

```{r}
library(ggplot2)
library(dplyr)

# Plot for all games
p1 <- ggplot(game, aes(x = Rating, y = activePlayers)) +
  geom_point(alpha = 0.4, color = "orange") +
  geom_smooth(method = "lm", color = "red", fill = "orange", se = TRUE) +
  labs(title = "Relationship between Game Rating and Number of Players for All Games",
       x = "Game Rating", y = "Number of Active Players") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
print(p1)
# Plot for Activision Blizzard games
p2 <- ggplot(game %>% filter(atvi_indi == 1), aes(x = Rating, y = activePlayers)) +
  geom_point(alpha = 0.4, color = "purple") +
  geom_smooth(method = "lm", color = "purple", fill = "purple", se = TRUE) +
  labs(title = "Relationship between Game Rating and Number of Players for ATVI Games",
       x = "Game Rating", y = "Number of Active Players") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
print(p2)
# Print the plots
#library(gridExtra)
#grid.arrange(p1, p2, ncol = 2)

```

```{r}
# Plot histograms
p1 <- ggplot(game, aes(x = Number.of.Reviews)) +
  geom_histogram(fill = "purple", color = "black") +
  theme_minimal() +
  labs(title = "Histogram of Number of Reviews", x = "Number of Reviews", y = "Frequency") +
  theme(plot.title = element_text(hjust = 0.5))

p2 <- ggplot(game, aes(x = Rating)) +
  geom_histogram(binwidth = 0.1, fill = "purple", color = "black") +
  theme_minimal() +
  labs(title = "Histogram of Ratings", x = "Rating", y = "Frequency") +
  theme(plot.title = element_text(hjust = 0.5))

p3 <- ggplot(game, aes(x = Wishlist)) +
  geom_histogram(fill = "purple", color = "black") +
  theme_minimal() +
  labs(title = "Histogram of Wishlists", x = "Number of Wishlists", y = "Frequency") +
  theme(plot.title = element_text(hjust = 0.5))

# Plot average per year
avg_reviews_per_year <- ggplot(game, aes(x = year, y = Number.of.Reviews)) +
  stat_summary(fun = mean, geom = "bar", fill = "orange", color = "black") +
  theme_minimal() +
  labs(title = "Average Number of Reviews per Year", x = "Year", y = "Average Number of Reviews") +
  theme(plot.title = element_text(hjust = 0.5))

avg_rating_per_year <- ggplot(game, aes(x = year, y = Rating)) +
  stat_summary(fun = mean, geom = "bar", fill = "orange", color = "black") +
  theme_minimal() +
  labs(title = "Average Rating per Year", x = "Year", y = "Average Rating") +
  theme(plot.title = element_text(hjust = 0.5))

avg_wishlists_per_year <- ggplot(game, aes(x = year, y = Wishlist)) +
  stat_summary(fun = mean, geom = "bar", fill = "orange", color = "black") +
  theme_minimal() +
  labs(title = "Average Number of Wishlists per Year", x = "Year", y = "Average Number of Wishlists") +
  theme(plot.title = element_text(hjust = 0.5))

p1
p2
p3
avg_reviews_per_year
avg_rating_per_year
avg_wishlists_per_year

```


```{r}
# Plot relationship between total sales and players
p <- ggplot() +
  geom_point(data = game, aes(x = activePlayers, y = total_sales, color = "Active Players")) +
  geom_smooth(data = game, aes(x = activePlayers, y = total_sales, color = "Active Players"), method = "lm", fill = "orange") +
  geom_point(data = game, aes(x = allPlayers, y = total_sales, color = "All Players")) +
  geom_smooth(data = game, aes(x = allPlayers, y = total_sales, color = "All Players"), method = "lm", fill = "pink") +
  labs(title = "Total Sales vs Players (2005-2023)",
       x = "Players",
       y = "Total Sales",
       color = "Player Type") +  # Label for the legend
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        text = element_text(size = 12),
        axis.title = element_text(size = 12, face = "bold")) +
  scale_x_continuous(limits = c(0, 1000000)) +
  scale_color_manual(values = c("Active Players" = "orange", "All Players" = "purple"))

p
```

```{r}
ggplot(game, aes(x = Rating)) + 
  geom_histogram(bins = 30, fill = "pink", color = "black") +
  theme_minimal() +
  scale_x_log10() +
  labs(title = "Distribution of Rating (2005-2023)", x = "Rating", y = "Frequency") +
  theme(plot.title = element_text(hjust = 0.5),
        text = element_text(size = 12),
        axis.title = element_text(size = 12, face = "bold"))
```



```{r}
# Games by Genre
ggplot(game, aes(x = genre, fill = genre)) +
  geom_bar(color = "black") +
  labs(title = "Number of Games by Genre (2005-2023)",
       x = "",  # Remove x-axis label
       y = "Count") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        text = element_text(size = 12),
        axis.title = element_text(size = 12, face = "bold"),
        legend.title = element_text(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.x = element_blank())

ggplot(game, aes(x = genre, y = allPlayers, fill = genre)) +
  geom_boxplot() +
  scale_y_log10() +  # Log scale for the y-axis to handle wide data range
  labs(title = "Total Players by Genre (2005-2023)",
       x = "Genre",
       y = "Total Players") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        text = element_text(size = 12),
        axis.title = element_text(size = 12, face = "bold"),
        legend.title = element_text(),
        axis.text.x = element_blank())
```

```{r}
# Distribution of games by console in the subset data
game_long <- game %>%
  tidyr::separate_rows(console, sep = " ") %>%
  filter(!is.na(console)) # Ensure console is not NA

console_counts <- game_long %>%
  group_by(console) %>%
  summarise(Count = n(),
            Total_activePlayers = sum(as.numeric(activePlayers), na.rm = TRUE),
            Total_Players = sum(as.numeric(allPlayers), na.rm = TRUE),
            Average_Rating = mean(as.numeric(Rating), na.rm = TRUE)) %>%
  ungroup()

top_consoles <- console_counts %>%
  top_n(10, Count) %>%
  arrange(desc(Count))
other <- console_counts %>%
  filter(!console %in% top_consoles$console) %>%
  summarise(console = "Other",
            Count = sum(Count),
            Total_activePlayers = sum(Total_activePlayers),
            Total_Players = sum(Total_Players),
            Average_Rating = mean(Average_Rating))
final_console_data <- bind_rows(top_consoles, other)
```


```{r}
# Pie chart
pie_data <- final_console_data %>%
  mutate(label = scales::percent(Count / sum(Count)))

ggplot(pie_data, aes(x = "", y = Count, fill = console)) +
  geom_col(color = "black") +
  geom_text(aes(label = Count),
            position = position_stack(vjust = 0.5)) +
  labs(title = "Number of Games across Consoles (2005-2023)") +
  theme_void() +
  coord_polar(theta = "y") +
  theme(plot.title = element_text(hjust = 0.5),
        text = element_text(size = 12),
        axis.title = element_text(size = 12, face = "bold"))
```

```{r}
ggplot(final_console_data, aes(x = reorder(console, Total_Players, decreasing = TRUE), y = Total_Players, fill = console)) +
  geom_bar(stat = "identity") +
  labs(title = "Total Players per Console (2005-2023)", x = "Console", y = "Total Players") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1))

game_long <- game %>%
  separate_rows(console, sep = " ") %>%
  mutate(allPlayers = as.numeric(allPlayers),
         year = as.numeric(year)) %>%
  filter(!is.na(console) & !is.na(allPlayers))

# Summarize data by console and year, considering only top consoles and others
yearly_console_data <- game_long %>%
  group_by(year, console) %>%
  summarise(Total_Players = sum(allPlayers, na.rm = TRUE), .groups = 'drop')

# Incorporating top 10 consoles logic and "Other"
top_consoles_list <- top_consoles$console  # Extract just the console names from the previous aggregation

yearly_console_data <- yearly_console_data %>%
  mutate(Grouped_Console = if_else(console %in% top_consoles_list, as.character(console), "Other")) %>%
  group_by(year, Grouped_Console) %>%
  summarise(Total_Players = sum(Total_Players), .groups = 'drop') %>%
  ungroup()

ggplot(yearly_console_data, aes(x = year, y = Total_Players, group = Grouped_Console, color = Grouped_Console)) +
  geom_line() +
  geom_point() +
  labs(title = "Total Players per Console Over Years (2005-2023)",
       x = "Year",
       y = "Total Players",
       color = "Console") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.title = element_blank())
```


```{r}
ggplot(final_console_data, aes(x = reorder(console, Average_Rating, decreasing = TRUE), y = Average_Rating, fill = console)) +
  geom_bar(stat = "identity") +
  labs(title = "Average Rating per Console", x = "Console", y = "Average Rating") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1))
```


```{r}
# Prepare the data: count games per year per genre
game_year_genre <- game %>%
  group_by(year, genre) %>%
  summarise(count = n(), .groups = 'drop')  # ensure you have year and genre as factors or appropriate format

# Plotting
ggplot(game_year_genre, aes(x = year, y = count, color = genre, group = genre)) +
  geom_line() +
  labs(title = "Trend of Games (by Counts) by Genre Per Year", x = "Year", y = "Counts") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels for better visibility
        legend.title = element_blank())
```


```{r}
game_year_genre <- game %>%
  group_by(year, genre) %>%
  summarise(Average_Rating = mean(Rating, na.rm = TRUE), .groups = 'drop')  
ggplot(game_year_genre, aes(x = year, y = Average_Rating, color = genre, group = genre)) +
  geom_line() +  # Draw lines connecting the average ratings
  labs(title = "Trend of Games (by Rating) by Genre Per Year",
       x = "Year",
       y = "Average Rating") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.title = element_blank())
```


```{r}
ggplot(game, aes(x = Release.Date, y = Rating, color = genre)) +  #Release.Date
  geom_point(alpha = 0.5, size = 2) +
  labs(title = "Game Ratings by Genre Over Years",
       x = "Release.Date",
       y = "Rating") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.title = element_text(face = "bold"),
        legend.title = element_text(face = "bold")) +
  guides(color = guide_legend(title = "Genre"))

```

## FDR

```{r}
# Selecting columns and creating model matrix
publisher_dummies <- model.matrix(~ publisher - 1, data = game)
developer_dummies <- model.matrix(~ developer - 1, data = game)
genre_dummies <- model.matrix(~ genre - 1, data = game)
# ip_dummies <- model.matrix(~ IP_Type - 1, data = game)
nReviews <- model.matrix(~ Number.of.Reviews, data = game)
nRating <- model.matrix(~ Rating, data = game)
wishlist_dummies <- model.matrix(~ Wishlist, data = game)
year_cont <- model.matrix(~ year_adj, data = game)
predictors_fdr <- cbind(year_cont, genre_dummies, publisher_dummies, wishlist_dummies, developer_dummies, nReviews, nRating)
predictors_fdr <- as(predictors_fdr, "sparseMatrix")
predictors_fdr <- as.data.frame(as.matrix(predictors_fdr))
```


```{r}
# Load necessary library
library(glmnet)
library(gamlr)
library(parallel)
library(Matrix)
source("fdr.R")


# Split the console column by spaces (or your actual delimiter)
consoles_list <- strsplit(game$console, split = " ")

# Flatten the list into a single vector and get unique console types
unique_consoles <- unique(unlist(consoles_list))

# Create a matrix where each row corresponds to a game and each column to a console, filled initially with 0s
console_matrix <- matrix(0, nrow = nrow(game), ncol = length(unique_consoles), dimnames = list(NULL, unique_consoles))
for (i in seq_along(consoles_list)) {
  console_matrix[i, consoles_list[[i]]] <- 1
}
console_sparse <- Matrix(console_matrix, sparse = TRUE)
console_df <- as.data.frame(as.matrix(console_sparse))
colnames(console_df) <- unique_consoles


# Fit the linear model
game_console <- cbind(game[, c("log_activePlayers", "year_adj", "Number.of.Reviews", "publisher", "genre", "Rating")], console_df)
game_console$year_adj <- scale(game_console$year_adj)
game_console$Rating <- scale(game_console$Rating)
game_console$Number_of_Reviews <- scale(game_console$Number.of.Reviews)

# game_console$Wishlist <- scale(game_console$Wishlist)

fit <- lm(log_activePlayers ~ ., data=game_console)
summary_fit <- summary(fit)
coefficients <- summary_fit$coefficients
mrgpvals <- coefficients[, 4]
source("fdr.R")
cutoff <- fdr_cut(mrgpvals,0.2,TRUE)
significant_indices <- mrgpvals <= cutoff
significant_coefficients <- coefficients[significant_indices, ]
significant_predictors <- rownames(coefficients)[significant_indices]

significant_predictors
kable(table(mrgpvals<cutoff))
cutoff
```


```{r}
# Fit the linear model
game_console <- cbind(game[, c("log_activePlayers", "year_adj", "genre", "Rating")])  # , console_df
game_console$year_adj <- scale(game_console$year_adj)
# game_console$Number_of_Reviews <- scale(game_console$Number.of.Reviews)
# game_console$Wishlist <- scale(game_console$Wishlist)

fit <- lm(log_activePlayers ~ ., data=game_console)
summary_fit <- summary(fit)
coefficients <- summary_fit$coefficients  # Retrieve the coefficients matrix
mrgpvals <- coefficients[, 4]
source("fdr.R")
cutoff <- fdr_cut(mrgpvals,0.1,TRUE)
significant_indices <- mrgpvals < cutoff
significant_coefficients <- coefficients[significant_indices, ]
significant_predictors <- rownames(coefficients)[significant_indices]

kable(table(mrgpvals<cutoff))
```
```{r}
# Fit the linear model
game_console <- cbind(game[, c("log_activePlayers", "year_adj", "publisher", "Rating")])  # , console_df
game_console$year_adj <- scale(game_console$year_adj)
# game_console$Number_of_Reviews <- scale(game_console$Number.of.Reviews)
# game_console$Wishlist <- scale(game_console$Wishlist)

fit <- lm(log_activePlayers ~ ., data=game_console)
summary_fit <- summary(fit)
coefficients <- summary_fit$coefficients  # Retrieve the coefficients matrix
mrgpvals <- coefficients[, 4]
source("fdr.R")
cutoff <- fdr_cut(mrgpvals,0.1,TRUE)
significant_indices <- mrgpvals < cutoff
significant_coefficients <- coefficients[significant_indices, ]
significant_predictors <- rownames(coefficients)[significant_indices]

kable(table(mrgpvals<cutoff))
```


```{r}
# Fit the linear model
game_console <- cbind(game[, c("log_activePlayers", "year_adj", "Rating")], console_df)
game_console$year_adj <- scale(game_console$year_adj)
#game_console$Number.of.Reviews <- scale(game_console$Number.of.Reviews)
#game_console$Wishlist <- scale(game_console$Wishlist)

fit <- lm(log_activePlayers ~ ., data=game_console)
summary_fit <- summary(fit)
coefficients <- summary_fit$coefficients  # Retrieve the coefficients matrix
mrgpvals <- coefficients[, 4]
source("fdr.R")
cutoff <- fdr_cut(mrgpvals,0.5,TRUE)
significant_indices <- mrgpvals <= cutoff
significant_coefficients <- coefficients[significant_indices, ]
significant_predictors <- rownames(coefficients)[significant_indices]

kable(table(mrgpvals<cutoff))
```

```{r}
# Linear Regression Model on genres
linear1 <- lm(log_activePlayers ~ year_adj + genre + Rating + Number.of.Reviews, data = game)  # + publisher + developer
summary(linear1)
```

```{r}
# linear2 <- lm(log_activePlayers ~ year * genre + Rating + Number.of.Reviews, data = game)
# summary(linear2)

# Create model matrices
train_matrix <- model.matrix(~ year * genre + Rating + Number.of.Reviews, data = train_data)
test_matrix <- model.matrix(~ year * genre + Rating + Number.of.Reviews, data = test_data)

# Fit the model using the matrix
linear2 <- lm(log_activePlayers ~ train_matrix - 1)  # '-1' avoids intercept since it's included in the matrix

# Predict using the corresponding test matrix
# Note: model.matrix aligns the levels automatically
test_preds <- predict(linear2, newdata = list(train_matrix = test_matrix))

```




```{r}
library(caret)

set.seed(123)  # for reproducibility
index <- createDataPartition(game$log_activePlayers, p = 0.8, list = FALSE)
train_data <- game[index, ]
test_data <- game[-index, ]

linear2 <- lm(log_activePlayers ~ year * genre + Rating + Number.of.Reviews, data = train_data)

# Predicting
train_preds <- predict(linear2, newdata = train_data)
test_preds <- predict(linear2, newdata = test_data)

# Calculate R-squared for training and testing
train_r2 <- summary(linear2)$r.squared
test_r2 <- 1 - sum((test_data$log_activePlayers - test_preds)^2) / sum((test_data$log_activePlayers - mean(test_data$log_activePlayers))^2)

library(ggplot2)

# Calculate residuals
train_res <- train_data$log_activePlayers - train_preds
test_res <- test_data$log_activePlayers - test_preds

# Prepare data for plotting
plot_data <- data.frame(
  Residuals = c(train_res, test_res),
  DataSet = factor(c(rep("Training", length(train_res)), rep("Testing", length(test_res))))
)

# Plot
ggplot(plot_data, aes(x = DataSet, y = Residuals^2)) +
  geom_boxplot() +
  labs(title = "Residual Squares (MSE) by Dataset", y = "Residuals Squared (MSE)", x = "") +
  theme_minimal()
```


## Does higher rating causes more number of players?
```{r}
library(gamlr)
library(Matrix)
library(dplyr)

set.seed(1234)

# Selecting columns and creating model matrix
game_console <- cbind(game[, c("year_adj", "Number.of.Reviews", "publisher", "genre")], console_df)
game_console$year_adj <- scale(game_console$year_adj)
game_console$Number_of_Reviews <- scale(game_console$Number.of.Reviews)
game$genre <- as.factor(game$genre)
game$publisher <- as.factor(game$publisher)
game$Rating <- as.numeric(game$Rating)

# Convert to a model matrix for Lasso
x <- model.matrix(~ ., data = game_console)
d <- game$Rating # Treatment
y <- game$log_activePlayers  # Outcome


## NAIVE LASSO regression
# Naive LASSO adds "treatment" as an extra covariate without giving it any special attention
naive <- gamlr(cbind(d,x),y)
coef(naive)["d",] # effect is AICc selected <0
plot(naive, main = "LASSO plot, Naive LASSO")
coef(naive, select=which.min(AICc(naive)))
# this is the effect of treatment, given everything else that LASSO keeps in the model the "everything else", however, might not include all the confounders :(


## Two stage LASSO
# First stage Lasso to predict treatment
treat <- gamlr(x, d, lambda.min.ratio=1e-4)
plot(treat, main = "LASSO plot, First Stage LASSO")  # Visualize the variable selection

# Predict the treatment (d_hat)
dhat <- predict(treat, x, type="response")
cor(drop(dhat),d)^2
plot(dhat,d,bty="n",pch=21,bg=8, main = "Relationship between d and d_hat") 

# Second stage Lasso to predict the outcome
causal <- gamlr(cbind(d, dhat, x), y, free=2, lmr=1e-4)
coef(causal)["d",]  # Extract the coefficient for the treatment

n <- nrow(x)
gamma <- c()  # Initialize storage for bootstrap results

for(b in 1:100){
    ib <- sample(1:n, n, replace=TRUE)
    xb <- x[ib, ]
    db <- d[ib]
    yb <- y[ib]
    treatb <- gamlr(xb, db, lambda.min.ratio=1e-3)
    dhatb <- predict(treatb, xb, type="response")
    fitb <- gamlr(cbind(db, dhatb, xb), yb, free=2)
    gamma <- c(gamma, coef(fitb)["db", ])
}

summary(gamma)  # Summarize the bootstrap results

mle <- glm(y ~ cbind(d, x)) 

# # get a standard error from Bootstrap
#
mean(gamma)+2*sd(gamma)
mean(gamma)-2*sd(gamma)
se <- summary(mle)$coef[2, 2]
se

sd(gamma)

```


```{r}
# Plot Boostrap
hist(gamma, freq = FALSE, main = "Bootstrapping Result, Causal Effect of Rating (gamma)", xlim = c(0, 0.1))

# Calculate the standard error and coefficient from your model mle

coef_estimate <- coef(mle)["cbind(d, x)d"]

# Add vertical lines
abline(v = coef_estimate, col = "orange", lwd = 2)  # Original estimate
text(coef_estimate, 0, labels = "mle Est.", pos = 3, cex = 0.8, col = "orange")
abline(v = mean(gamma), col = "purple", lwd = 2)  # gamma mean
text(mean(gamma), 0, labels = "Boostrap Est.", pos = 3, cex = 0.8, col = "purple")

# Confidence interval from mle
abline(v = coef_estimate + 2 * se, col = "orange", lwd = 2, lty = "dashed")  # Upper mle CI
text(coef_estimate + 2 * se, 0, labels = "mle CI", pos = 3, cex = 0.8, col = "orange")
abline(v = coef_estimate - 2 * se, col = "orange", lwd = 2, lty = "dashed")  # Lower mle CI
text(coef_estimate - 2 * se, 0, labels = "mle CI", pos = 3, cex = 0.8, col = "orange")

# Confidence interval from bootstrap
abline(v = quantile(gamma, 0.025), col = "purple", lwd = 2, lty = "dashed")
text(quantile(gamma, 0.025), 0, labels = "Bootstrap CI", pos = 3, cex = 0.8, col = "purple")
abline(v = quantile(gamma, 0.975), col = "purple", lwd = 2, lty = "dashed")
text(quantile(gamma, 0.975), 0, labels = "Bootstrap CI", pos = 3, cex = 0.8, col = "purple")

```


## LASSO

```{r}
# Selecting columns and creating model matrix
game_console <- cbind(game[, c("year_adj", "Number.of.Reviews", "publisher", "genre", "Rating")], console_df)
game_console$year_adj <- scale(game_console$year_adj)
game_console$Number_of_Reviews <- scale(game_console$Number.of.Reviews)
game_console$Rating <- as.numeric(game_console$Rating)

# Convert to a model matrix for Lasso
x <- model.matrix(~ ., data = game_console)
```


```{r}
library(gamlr)
lasso3.cv <- cv.gamlr(x, game$log_activePlayers, lambda.min.ratio = 1e-3,
                      family = "gaussian", verb = TRUE)
# par(mfrow=c(1,2))
# plot(lasso3.cv$gamlr)
# plot(lasso3.cv)
sum(coef(lasso3.cv)!=0) # 1se
sum(coef(lasso3.cv, s="min")!=0) # min
sum(coef(lasso3.cv$gamlr)!=0) # AICc
```

```{r}
set.seed(1234)
# CROSS-VALIDATION
coef(lasso3.cv) ## 1se rule; see ?cv.gamlr
coef(lasso3.cv, select="min") ## min cv selection

## log lambdas selected under various criteria
log_lambdas <- function(cv_obj) {
  gamlr_obj <- cv_obj$gamlr
  n_lambdas <- length(gamlr_obj$lambda)
  n <- nrow(cv_obj$gamlr$x)
  
  # Calculate AIC, AICc, and BIC
  aic_values <- AIC(gamlr_obj)
  aicc_values <- AICc(gamlr_obj)
  bic_values <- BIC(gamlr_obj)
  
  # Extracting lambda values
  lambda_aicc <- gamlr_obj$lambda[which.min(aicc_values)]
  lambda_aic <- gamlr_obj$lambda[which.min(aic_values)]
  lambda_bic <- gamlr_obj$lambda[which.min(bic_values)]
  lambda_min <- cv_obj$lambda.min
  lambda_1se <- cv_obj$lambda.1se
  
  return(list(lambda_aicc = lambda_aicc,
              lambda_aic = lambda_aic,
              lambda_bic = lambda_bic,
              lambda_min = lambda_min,
              lambda_1se = lambda_1se))
}

lambdas <- log_lambdas(lasso3.cv)

# Log lambdas
log(lambdas$lambda_aicc)
log(lambdas$lambda_aic)
log(lambdas$lambda_bic)
log(lambdas$lambda_min)
log(lambdas$lambda_1se)

# Plot the cross-validation results with annotations
# par(mfrow = c(1, 2))

# Plot the LASSO path from gamlr
plot(lasso3.cv$gamlr, main = "LASSO Path with AIC, AICc, BIC, and CV")

# Adding vertical lines for the different criteria
abline(v = log(lambdas$lambda_aicc), col = "black", lty = 2)
abline(v = log(lambdas$lambda_aic), col = "purple", lty = 2)
abline(v = log(lambdas$lambda_bic), col = "green", lty = 2)
abline(v = log(lambdas$lambda_min), col = "orange", lty = 2)
abline(v = log(lambdas$lambda_1se), col = "blue", lty = 2)

legend("topright", bty = "n", lwd = 1, 
       col = c("black", "purple", "green", "orange", "blue"),
       legend = c("AICc", "AIC", "BIC", "CV.min", "CV.1se"))

# Plot the cross-validation plot
plot(lasso3.cv, main = "Cross-Validation")
```


```{r}
y_pred <- predict(lasso3.cv, x, select = "min")
rss <- sum((game$log_activePlayers - y_pred)^2)
tss <- sum((game$log_activePlayers - mean(game$log_activePlayers))^2)
r_squared <- 1 - rss/tss

# Print R-squared
print(r_squared)

```






```{r}
# Create a dataframe with word frequencies
# Load necessary libraries
library(dplyr)
library(tidyr)
library(stringr)
```


```{r}
# Tree
library(rpart)
tree_model <- rpart(activePlayers ~ genre + publisher + developer + Rating + Number.of.Reviews, data = game)
plot(tree_model)
text(tree_model, pretty = 0)
```


```{r}
# Define filler words to be removed
filler_words <- c("the", "is", "a", "has", "have", "and", "of", "in", "to", "for", "with", "on", "that", "lets","as", "out", "by","from", "this", "be", "an", "most", "each", "one", "player","take","game","gameplay", "them", "at", "how", "may", "from","players","v", "or", "so", "you","what","while","other","into","through", "are", "can", "will", "which", "t", "who", "where", "also", "his", "her", "their", "they", "up", "he", "she", "its", "it", "includes", "include","your", "you", "all","���������", "s", "any", "ll", "was", "but", "if", "there", "these")

# Function to clean and tokenize text, removing filler words
clean_and_tokenize <- function(text, filler_words) {
  text_clean <- tolower(enc2utf8(text))
  # text_clean <- tolower(gsub("[^\\x01-\\x7F]", "", text_clean))
  text_clean <- str_replace_all(text_clean, "[[:punct:]]", " ")
  words <- unlist(strsplit(text_clean, "\\s+"))
  words <- words[!words %in% filler_words]
  return(words)
}

# Apply the function to the Summary column
game$Summary_clean <- sapply(game$Summary, function(x) paste(clean_and_tokenize(x, filler_words), collapse = " "))

# Create a corpus from the cleaned summaries
corpus <- Corpus(VectorSource(game$Summary_clean))
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, Inf)))
tdm_matrix <- as.matrix(tdm)
```


```{r}
# Get word frequencies
word_freq <- sort(rowSums(tdm_matrix), decreasing = TRUE)
word_freq_df <- data.frame(word = names(word_freq), frequency = word_freq)

# Filter for significant words
sig_word_sum <- word_freq_df %>% filter(frequency > 5)
sig_word_sum <- sig_word_sum %>% arrange(word) %>% slice(-1:-19)
sig_word_sum <- as.character(sig_word_sum$word)

# Convert to sparse matrix
tdm_filtered <- tdm_matrix[sig_word_sum, ]
tdm_sparse <- as(t(tdm_filtered), "sparseMatrix")
```


```{r}
# Filter the term-document matrix to keep only significant words
tdm_filtered <- tdm_matrix[sig_word_sum, ]
tdm_sparse <- as(t(tdm_filtered), "sparseMatrix")

## Combine x 
genre_dummies <- model.matrix(~ genre - 1, data = game)
year_cont <- model.matrix(~ year_adj, data = game)
publisher_dummies <- model.matrix(~ publisher - 1, data = game)
game$Rating <- as.numeric(game$Rating)
rating_cont <- model.matrix(~ Rating, data = game)
ip_dummies <- model.matrix(~ IP_Type - 1, data = game)
x <- cbind(genre_dummies, year_cont, publisher_dummies, rating_cont, ip_dummies, tdm_sparse, console_sparse)
x <- as(x, "sparseMatrix")
```


```{r}
## Plot word cloud

library(tm)
library(wordcloud)
library(dplyr)
library(RColorBrewer)

# Filter the term-document matrix to keep only significant words
tdm_filtered <- tdm_matrix[sig_word_sum, ]

# Create the word cloud
word_freq_filtered <- sort(rowSums(tdm_filtered), decreasing = TRUE)
wordcloud(names(word_freq_filtered), freq = word_freq_filtered, min.freq = 1, scale = c(4, 0.5), colors = brewer.pal(8, "Dark2"))
```



```{r}
# Run lasso on genre, summary words, and year
lasso_sum <- gamlr(x, game$log_activePlayers, standardize=TRUE, family = "gaussian", lambda.min.ratio=1e-3)
plot(lasso_sum, main="LASSO plot, Summary Words+Other Controls")

# in-sample R2
1- lasso_sum$deviance[which.min(AICc(lasso_sum))]/lasso_sum$deviance[1]

# Check coefficients
lasso_sum_coef <- coef(lasso_sum, select=which.min(AICc(lasso_sum)))
sum(lasso_sum_coef!=0)

lasso_sum_coef <- as.data.frame(as.matrix(lasso_sum_coef))
colnames(lasso_sum_coef) <- c("Coefficient")
lasso_sum_coef$Feature <- rownames(lasso_sum_coef)
rownames(lasso_sum_coef) <- NULL
lasso_sum_coef_sig <- lasso_sum_coef %>% filter(Coefficient != 0)%>% arrange(desc(Coefficient))
lasso_sum_coef_sig[-1, ]
```

```{r}
library(gamlr)
lasso_sum.cv <- cv.gamlr(x, game$log_activePlayers, lambda.min.ratio = 1e-3,
                      family = "gaussian", verb = TRUE)
# par(mfrow=c(1,2))
# plot(lasso3.cv$gamlr)
# plot(lasso3.cv)
sum(coef(lasso_sum.cv)!=0) # 1se
sum(coef(lasso_sum.cv, s="min")!=0) # min
sum(coef(lasso_sum.cv$gamlr)!=0) # AICc
```

```{r}
set.seed(1234)
# CROSS-VALIDATION
coef(lasso_sum.cv) ## 1se rule; see ?cv.gamlr
coef(lasso_sum.cv, select="min") ## min cv selection

## log lambdas selected under various criteria
log_lambdas <- function(cv_obj) {
  gamlr_obj <- cv_obj$gamlr
  n_lambdas <- length(gamlr_obj$lambda)
  n <- nrow(cv_obj$gamlr$x)
  
  # Calculate AIC, AICc, and BIC
  aic_values <- AIC(gamlr_obj)
  aicc_values <- AICc(gamlr_obj)
  bic_values <- BIC(gamlr_obj)
  
  # Extracting lambda values
  lambda_aicc <- gamlr_obj$lambda[which.min(aicc_values)]
  lambda_aic <- gamlr_obj$lambda[which.min(aic_values)]
  lambda_bic <- gamlr_obj$lambda[which.min(bic_values)]
  lambda_min <- cv_obj$lambda.min
  lambda_1se <- cv_obj$lambda.1se
  
  return(list(lambda_aicc = lambda_aicc,
              lambda_aic = lambda_aic,
              lambda_bic = lambda_bic,
              lambda_min = lambda_min,
              lambda_1se = lambda_1se))
}

lambdas <- log_lambdas(lasso_sum.cv)

# Log lambdas
log(lambdas$lambda_aicc)
log(lambdas$lambda_aic)
log(lambdas$lambda_bic)
log(lambdas$lambda_min)
log(lambdas$lambda_1se)

# Plot the LASSO path from gamlr
plot(lasso_sum.cv$gamlr, main = "Summary LASSO Path with AIC, AICc, BIC, and CV")

# Adding vertical lines for the different criteria
abline(v = log(lambdas$lambda_aicc), col = "black", lty = 2)
abline(v = log(lambdas$lambda_aic), col = "purple", lty = 2)
abline(v = log(lambdas$lambda_bic), col = "green", lty = 2)
abline(v = log(lambdas$lambda_min), col = "orange", lty = 2)
abline(v = log(lambdas$lambda_1se), col = "blue", lty = 2)

legend("topright", bty = "n", lwd = 1, 
       col = c("black", "purple", "green", "orange", "blue"),
       legend = c("AICc", "AIC", "BIC", "CV.min", "CV.1se"))

# Plot the cross-validation plot
plot(lasso_sum.cv, main = "Cross-Validation")
```

## Binomial - very bad performance
```{r}
library(gamlr)
lasso_sum.cv <- cv.gamlr(x, game$activePlayers_dummy, lambda.min.ratio = 1e-3,
                      family = "binomial", verb = TRUE)
# par(mfrow=c(1,2))
# plot(lasso3.cv$gamlr)
# plot(lasso3.cv)
sum(coef(lasso_sum.cv)!=0) # 1se
sum(coef(lasso_sum.cv, s="min")!=0) # min
sum(coef(lasso_sum.cv$gamlr)!=0) # AICc
```

```{r}
set.seed(1234)
# CROSS-VALIDATION
coef(lasso_sum.cv) ## 1se rule; see ?cv.gamlr
coef(lasso_sum.cv, select="min") ## min cv selection

## log lambdas selected under various criteria
log_lambdas <- function(cv_obj) {
  gamlr_obj <- cv_obj$gamlr
  n_lambdas <- length(gamlr_obj$lambda)
  n <- nrow(cv_obj$gamlr$x)
  
  # Calculate AIC, AICc, and BIC
  aic_values <- AIC(gamlr_obj)
  aicc_values <- AICc(gamlr_obj)
  bic_values <- BIC(gamlr_obj)
  
  # Extracting lambda values
  lambda_aicc <- gamlr_obj$lambda[which.min(aicc_values)]
  lambda_aic <- gamlr_obj$lambda[which.min(aic_values)]
  lambda_bic <- gamlr_obj$lambda[which.min(bic_values)]
  lambda_min <- cv_obj$lambda.min
  lambda_1se <- cv_obj$lambda.1se
  
  return(list(lambda_aicc = lambda_aicc,
              lambda_aic = lambda_aic,
              lambda_bic = lambda_bic,
              lambda_min = lambda_min,
              lambda_1se = lambda_1se))
}

lambdas <- log_lambdas(lasso_sum.cv)

# Log lambdas
log(lambdas$lambda_aicc)
log(lambdas$lambda_aic)
log(lambdas$lambda_bic)
log(lambdas$lambda_min)
log(lambdas$lambda_1se)

# Plot the LASSO path from gamlr
plot(lasso_sum.cv$gamlr, main = "Summary LASSO Path with AIC, AICc, BIC, and CV")

# Adding vertical lines for the different criteria
abline(v = log(lambdas$lambda_aicc), col = "black", lty = 2)
abline(v = log(lambdas$lambda_aic), col = "purple", lty = 2)
abline(v = log(lambdas$lambda_bic), col = "green", lty = 2)
abline(v = log(lambdas$lambda_min), col = "orange", lty = 2)
abline(v = log(lambdas$lambda_1se), col = "blue", lty = 2)

legend("topright", bty = "n", lwd = 1, 
       col = c("black", "purple", "green", "orange", "blue"),
       legend = c("AICc", "AIC", "BIC", "CV.min", "CV.1se"))

# Plot the cross-validation plot
plot(lasso_sum.cv, main = "Cross-Validation")
```

## OOS R2 LASSO

```{r}
library(glmnet)
library(gamlr)
library(ggplot2)
set.seed(1234)

## Combine x 
genre_dummies <- model.matrix(~ genre - 1, data = game)
year_cont <- model.matrix(~ year_adj, data = game)
publisher_dummies <- model.matrix(~ publisher - 1, data = game)
game$Rating <- as.numeric(game$Rating)
rating_cont <- model.matrix(~ Rating, data = game)
ip_dummies <- model.matrix(~ IP_Type - 1, data = game)
x <- cbind(genre_dummies, year_cont, publisher_dummies, rating_cont, ip_dummies, tdm_sparse, console_sparse)
x <- as(x, "sparseMatrix")

# Function to split data and calculate OOS R² for LASSO
calculate_oos_r2 <- function(x, y, train_fraction = 0.7, n_splits = 20) {
  oos_r2_values <- numeric(n_splits)
  
  for (i in 1:n_splits) {
    # Split data into training and testing sets
    train_indices <- sample(1:nrow(x), size = floor(train_fraction * nrow(x)))
    test_indices <- setdiff(1:nrow(x), train_indices)
    
    x_train <- x[train_indices, ]
    y_train <- y[train_indices]
    x_test <- x[test_indices, ]
    y_test <- y[test_indices]
    
    # Fit LASSO model
    lasso_model <- cv.gamlr(x_train, y_train, lambda.min.ratio = 1e-3, family = "gaussian")
    
    # Predict on test set using the lambda with minimum cross-validated error
    y_pred <- predict(lasso_model, x_test, select = "min")
    
    # Calculate OOS R²
    rss <- sum((y_test - y_pred)^2)
    tss <- sum((y_test - mean(y_test))^2)
    oos_r2 <- 1 - rss/tss
    
    oos_r2_values[i] <- oos_r2
  }
  
  return(oos_r2_values)
}

# Apply function to your data
x <- as.matrix(x)  # Ensure x is a matrix
y <- game$log_activePlayers
oos_r2_lasso <- calculate_oos_r2(x, y)

# Repeat for other models and collect their OOS R² values
# For brevity, only LASSO is shown here. Extend similarly for other models.


# Combine OOS R² values into a data frame
model_names <- rep(c("LASSO"), each = 20)  # Extend with other model names
oos_r2_values <- c(oos_r2_lasso)  # Combine with other OOS R² values

results_df <- data.frame(model = model_names, OOS_R2 = oos_r2_values)

# Plot using ggplot2
library(ggplot2)

ggplot(results_df, aes(x = model, y = OOS_R2)) +
  geom_boxplot(fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "OOS R² Comparison", x = "Model", y = "OOS R²") +
  theme(plot.title = element_text(hjust = 0.5))

```



```{r}
set.seed(1234)

# Function to split data and calculate OOS R² for LASSO using AICc and CV.min
calculate_oos_r2 <- function(x, y, train_fraction = 0.7, n_splits = 20) {
  oos_r2_aicc <- numeric(n_splits)
  oos_r2_cv_min <- numeric(n_splits)
  
  for (i in 1:n_splits) {
    # Split data into training and testing sets
    train_indices <- sample(1:nrow(x), size = floor(train_fraction * nrow(x)))
    test_indices <- setdiff(1:nrow(x), train_indices)
    
    x_train <- x[train_indices, ]
    y_train <- y[train_indices]
    x_test <- x[test_indices, ]
    y_test <- y[test_indices]
    
    # Fit LASSO model with gamlr
    lasso_model <- gamlr(x_train, y_train, lambda.min.ratio = 1e-3, family = "gaussian")
    lasso_cv <- cv.gamlr(x_train, y_train, lambda.min.ratio = 1e-3, family = "gaussian")
    
    # Select lambda using AICc
    aicc_values <- AICc(lasso_model)
    lambda_aicc <- lasso_model$lambda[which.min(aicc_values)]
    
    # Predict on test set using the lambda with minimum AICc
    y_pred_aicc <- predict(lasso_model, newdata = x_test, lambda = lambda_aicc)
    
    # Calculate OOS R² for AICc
    rss_aicc <- sum((y_test - y_pred_aicc)^2)
    tss_aicc <- sum((y_test - mean(y_test))^2)
    oos_r2_aicc[i] <- 1 - rss_aicc/tss_aicc
    
    # Predict on test set using the lambda with minimum cross-validated error
    y_pred_cv_min <- predict(lasso_cv, newdata = x_test, select = "min")
    
    # Calculate OOS R² for CV.min
    rss_cv_min <- sum((y_test - y_pred_cv_min)^2)
    tss_cv_min <- sum((y_test - mean(y_test))^2)
    oos_r2_cv_min[i] <- 1 - rss_cv_min/tss_cv_min
  }
  
  return(list(oos_r2_aicc = oos_r2_aicc, oos_r2_cv_min = oos_r2_cv_min))
}

# Apply function to your data
x <- as.matrix(x)  # Ensure x is a matrix
y <- game$log_activePlayers
oos_r2_results <- calculate_oos_r2(x, y)

# Combine OOS R² values into a data frame
model_names <- rep(c("LASSO_AICc", "LASSO_CVmin"), each = 20)
oos_r2_values <- c(oos_r2_results$oos_r2_aicc, oos_r2_results$oos_r2_cv_min)

results_df <- data.frame(model = model_names, OOS_R2 = oos_r2_values)

```

## Unpruned tree
```{r}
library(rpart)
library(rpart.plot)

genre_df <- as.data.frame(model.matrix(~ genre - 1, data = game))
publisher_df <- as.data.frame(model.matrix(~ publisher - 1, data = game))
ip_df <- model.matrix(~ IP_Type - 1, data = game)
tdm_df <- as.data.frame(as.matrix(tdm_sparse))
x_tree <- cbind(game[, c("year_adj", "Rating")], genre_df, publisher_df, ip_df, console_df) # , tdm_df
x_tree$year_adj <- scale(x_tree$year_adj)
x_tree$Rating <- scale(x_tree$Rating)

# Fit the unpruned decision tree model
tree_model <- rpart(game$log_activePlayers ~ ., data = data.frame(x_tree), control = rpart.control(cp = 0))

# Plot the unpruned tree
rpart.plot(tree_model, main = "Unpruned Decision Tree")

pdf("unpruned_tree_plot.pdf", width = 8, height = 6)
rpart.plot(tree_model, main = "Unpruned Decision Tree")
dev.off()


# Function to split data and calculate OOS R² for the unpruned tree
calculate_oos_r2_tree <- function(x, y, train_fraction = 0.7, n_splits = 20) {
  oos_r2_values <- numeric(n_splits)
  
  for (i in 1:n_splits) {
    # Split data into training and testing sets
    train_indices <- sample(1:nrow(x), size = floor(train_fraction * nrow(x)))
    test_indices <- setdiff(1:nrow(x), train_indices)
    
    x_train <- x[train_indices, ]
    y_train <- y[train_indices]
    x_test <- x[test_indices, ]
    y_test <- y[test_indices]
    
    # Fit the unpruned decision tree model
    tree_model <- rpart(y_train ~ ., data = data.frame(x_train), control = rpart.control(cp = 0))
    
    # Predict on the test set
    y_pred <- predict(tree_model, newdata = data.frame(x_test))
    
    # Calculate OOS R²
    rss <- sum((y_test - y_pred)^2)
    tss <- sum((y_test - mean(y_test))^2)
    oos_r2 <- 1 - rss/tss
    
    oos_r2_values[i] <- oos_r2
  }
  
  return(oos_r2_values)
}

# Apply function to your data
oos_r2_tree <- calculate_oos_r2_tree(x_tree, game$log_activePlayers)

# Combine OOS R² values into a data frame
model_names <- rep(c("LASSO_AICc", "LASSO_CVmin", "Tree_Unpruned"), each = 20)
oos_r2_values <- c(oos_r2_results$oos_r2_aicc, oos_r2_results$oos_r2_cv_min, oos_r2_tree)

results_df <- data.frame(model = model_names, OOS_R2 = oos_r2_values)

# Plot using ggplot2
ggplot(results_df, aes(x = model, y = OOS_R2)) +
  geom_boxplot(fill = "purple", color = "black") +
  theme_minimal() +
  labs(title = "OOS R² Comparison for LASSO and Unpruned Tree", x = "Model", y = "OOS R²") +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
# Prune the tree based on the cp that minimizes cross-validated error
cp_table <- printcp(tree_model)
best_cp <- cp_table[which.min(cp_table[, "xerror"]), "CP"]
pruned_tree <- prune(tree_model, cp = best_cp)

# Plot the pruned tree
rpart.plot(pruned_tree, main = "Pruned Decision Tree")

pdf("pruned_tree_plot.pdf", width = 8, height = 6)
rpart.plot(pruned_tree, main = "Pruned Decision Tree")
dev.off()

# Function to calculate OOS R² for a given tree model
calculate_oos_r2_tree <- function(x, y, tree_control, train_fraction = 0.7, n_splits = 20) {
  oos_r2_values <- numeric(n_splits)
  
  for (i in 1:n_splits) {
    # Split data into training and testing sets
    train_indices <- sample(1:nrow(x), size = floor(train_fraction * nrow(x)))
    test_indices <- setdiff(1:nrow(x), train_indices)
    
    x_train <- x[train_indices, ]
    y_train <- y[train_indices]
    x_test <- x[test_indices, ]
    y_test <- y[test_indices]
    
    # Fit the tree model
    tree_model <- rpart(y_train ~ ., data = data.frame(x_train), control = tree_control)
    
    # Predict on the test set
    y_pred <- predict(tree_model, newdata = data.frame(x_test))
    
    # Calculate OOS R²
    rss <- sum((y_test - y_pred)^2)
    tss <- sum((y_test - mean(y_test))^2)
    oos_r2 <- 1 - rss/tss
    
    oos_r2_values[i] <- oos_r2
  }
  
  return(oos_r2_values)
}

# Apply function to your data for unpruned tree
unpruned_control <- rpart.control(cp = 0)
oos_r2_unpruned <- calculate_oos_r2_tree(x_tree, game$log_activePlayers, unpruned_control)

# Apply function to your data for pruned tree
pruned_control <- rpart.control(cp = best_cp)
oos_r2_pruned <- calculate_oos_r2_tree(x_tree, game$log_activePlayers, pruned_control)

# Combine OOS R² values into a data frame
model_names <- rep(c("Unpruned Tree", "Pruned Tree"), each = 20)
oos_r2_values <- c(oos_r2_unpruned, oos_r2_pruned)

results_df <- data.frame(model = model_names, OOS_R2 = oos_r2_values)

# Plot using ggplot2
ggplot(results_df, aes(x = model, y = OOS_R2)) +
  geom_boxplot(fill = "purple", color = "black") +
  theme_minimal() +
  labs(title = "OOS R² Comparison for Unpruned and Pruned Trees", x = "Model", y = "OOS R²") +
  theme(plot.title = element_text(hjust = 0.5))

```


## Random Forest
```{r}
# Convert factors to numeric
x_tree <- cbind(game[, c("year_adj", "Rating")], genre_df, publisher_df, ip_df, console_df)
x_tree$year_adj <- scale(x_tree$year_adj)
x_tree$Rating <- scale(x_tree$Rating)
x_tree <- data.frame(lapply(x_tree, function(x) if(is.factor(x)) as.numeric(x) else x))

# Fit the Random Forest model
rf_model <- randomForest(game$log_activePlayers ~ ., data = data.frame(x_tree), ntree = 500, importance = TRUE)
print(rf_model)

# Plot the importance of variables
png("variable_importance_plot.png", width = 1200, height = 800)
par(mar = c(5, 15, 4, 2) + 0.1)
varImpPlot(rf_model, main = "Variable Importance Plot", n.var = min(30, nrow(rf_model$importance)))
dev.off()


# Function to split data and calculate OOS R² for Random Forest
calculate_oos_r2_rf <- function(x, y, train_fraction = 0.7, n_splits = 20) {
  oos_r2_values <- numeric(n_splits)
  
  for (i in 1:n_splits) {
    # Split data into training and testing sets
    train_indices <- sample(1:nrow(x), size = floor(train_fraction * nrow(x)))
    test_indices <- setdiff(1:nrow(x), train_indices)
    
    x_train <- x[train_indices, ]
    y_train <- y[train_indices]
    x_test <- x[test_indices, ]
    y_test <- y[test_indices]
    
    # Fit the Random Forest model
    rf_model <- randomForest(y_train ~ ., data = data.frame(x_train), ntree = 500)
    
    # Predict on the test set
    y_pred <- predict(rf_model, newdata = data.frame(x_test))
    
    # Calculate OOS R2
    rss <- sum((y_test - y_pred)^2)
    tss <- sum((y_test - mean(y_test))^2)
    oos_r2 <- 1 - rss/tss
    
    oos_r2_values[i] <- oos_r2
  }
  
  return(oos_r2_values)
}


x <- as.matrix(x_tree)
y <- game$log_activePlayers
oos_r2_rf <- calculate_oos_r2_rf(x, y)
```


```{r}
# Combine OOS R² values into a data frame
model_names <- rep(c("LASSO_AICc", "LASSO_CVmin", "Tree_Unpruned", "Tree_Pruned", "RF"), each = 20)
oos_r2_values <- c(oos_r2_results$oos_r2_aicc, oos_r2_results$oos_r2_cv_min, oos_r2_tree, oos_r2_pruned, oos_r2_rf)

results_df <- data.frame(model = model_names, OOS_R2 = oos_r2_values)

# Plot using ggplot2
ggplot(results_df, aes(x = model, y = OOS_R2)) +
  geom_boxplot(fill = "purple", color = "black") +
  theme_minimal() +
  labs(title = "OOS R² Comparison for LASSO, Trees, and Random Forest", x = "Model", y = "OOS R²") +
  theme(plot.title = element_text(hjust = 0.5))

```



## Additional Processing

```{r}
game$series <- sapply(str_split(game$Title, "\\s+"), function(words) {
  paste(words[1:min(length(words), 2)], collapse = " ")
})

# 计算每个系列的出现次数
series_counts <- game %>%
  group_by(series, publisher) %>%
  summarise(Count = n(), .groups = 'drop')

# 将 series_counts 加入到原数据框中
game <- game %>%
  left_join(series_counts, by = c("series", "publisher"))

# 标记大IP和小IP
game <- game %>%
  mutate(IP_Type = case_when(
    Count >= 6 ~ "Big IP",
    Count >= 4 ~ "Medium IP",
    Count >= 2 ~ "Small IP",
    TRUE ~ "Not IP"
  ))


```


```{r}
game <- game %>%
  mutate(activePlayersGroup = cut(activePlayers, breaks = c(-Inf, 120000, 170000, Inf), labels = c("Low", "Medium", "High")))

ip_data <- game %>%
  filter(IP_Type %in% c("Big IP", "Small IP","Medium IP"))

ip_data$activePlayers <- as.numeric(ip_data$activePlayers)
table(ip_data$IP_Type)
```

```{r}
ip_genre_table <- table(ip_data$IP_Type, ip_data$genre)

```

```{r}
ip_data$activePlayers <- as.numeric(ip_data$activePlayers)

summary(ip_data$activePlayers)

ip_avg_active_players <- ip_data %>%
  group_by(IP_Type) %>%
  summarise(
    avg_activePlayers = mean(activePlayers, na.rm = TRUE),
    max_activePlayers = max(activePlayers, na.rm = TRUE),
    min_activePlayers = min(activePlayers, na.rm = TRUE),
    count = n()
  )

# 查看结果
print(ip_avg_active_players)
```

```{r}
library(tidyverse)
library(rpart)
library(rpart.plot)

tree_model <- rpart(IP_Type ~ activePlayers + genre, data = ip_data, method = "class")

summary(tree_model)

rpart.plot(tree_model, type = 3, extra = 101, fallen.leaves = TRUE, main = "Decision Tree for IP Type by Active Players and Genre",
           cex = 0.4, # 调整字体大小
           tweak = 1.2, # 调整图形元素大小
           box.palette = "RdBu", shadow.col = "gray", nn = TRUE)
```

```{r}
library(glmnet)
library(caret)
game <- game %>%
  filter(!is.na(Rating) & !is.na(genre) & !is.na(IP_Type))
game$IP_Type <- factor(game$IP_Type, levels = c("Big IP", "Medium IP", "Small IP", "Not IP"))
```


## KNN


```{r}
set.seed(1234)
# Selecting columns and creating model matrix
publisher_dummies <- model.matrix(~ publisher - 1, data = game)
developer_dummies <- model.matrix(~ developer - 1, data = game)
genre_dummies <- model.matrix(~ genre - 1, data = game)
developer_dummies <- model.matrix(~ developer - 1, data = game)
ip_dummies <- model.matrix(~ IP_Type - 1, data = game)
year_cont <- model.matrix(~ year_adj, data = game)
rating_cont <- model.matrix(~ Rating, data = game)

predictors_knn <- cbind(year_cont, genre_dummies, rating_cont, publisher_dummies, developer_dummies, ip_dummies, tdm_sparse, console_sparse)
predictors_knn <- as(predictors_knn, "sparseMatrix")
```


```{r}
set.seed(1234)
quantiles <- quantile(game$activePlayers, probs=c(0, 0.5, 1), na.rm=TRUE)
y <- cut(game$activePlayers, breaks=quantiles, include.lowest=TRUE, labels=c(0, 1))
train <- createDataPartition(y, p = 0.5, list = FALSE)

## Compare K = 10, 20, 40
set.seed(1234)
nearest10 <- class::knn(train=predictors_knn[train,], test=predictors_knn[-train,], cl=y[train], prob=TRUE, k=10)  
nearest20 <- class::knn(train=predictors_knn[train,], test=predictors_knn[-train,], cl=y[train], prob=TRUE, k=20)  
nearest40 <- class::knn(train=predictors_knn[train,], test=predictors_knn[-train,], cl=y[train], prob=TRUE, k=40)  
data.frame(y[-train],nearest10,nearest20, nearest40)

confusion_matrix_10 <- table(y[-train], nearest10)
confusion_matrix_20 <- table(y[-train], nearest20)
confusion_matrix_40 <- table(y[-train], nearest40)
fp_rate_10 <- confusion_matrix_10[1,2] / sum(confusion_matrix_10[,2]) # False Positive Rate
fn_rate_10 <- confusion_matrix_10[2,1] / sum(confusion_matrix_10[,1]) # False Negative Rate
sensitivity_10 <- confusion_matrix_10[2,2] / sum(confusion_matrix_10[2,]) # Sensitivity
specificity_10 <- confusion_matrix_10[1,1] / sum(confusion_matrix_10[1,]) # Specificity

# Calculate performance metrics for k = 20
fp_rate_20 <- confusion_matrix_20[1,2] / sum(confusion_matrix_20[,2]) # False Positive Rate
fn_rate_20 <- confusion_matrix_20[2,1] / sum(confusion_matrix_20[,1]) # False Negative Rate
sensitivity_20 <- confusion_matrix_20[2,2] / sum(confusion_matrix_20[2,]) # Sensitivity
specificity_20 <- confusion_matrix_20[1,1] / sum(confusion_matrix_20[1,]) # Specificity

# Calculate performance metrics for k = 40
fp_rate_40 <- confusion_matrix_40[1,2] / sum(confusion_matrix_40[,2]) # False Positive Rate
fn_rate_40 <- confusion_matrix_40[2,1] / sum(confusion_matrix_40[,1]) # False Negative Rate
sensitivity_40 <- confusion_matrix_40[2,2] / sum(confusion_matrix_40[2,]) # Sensitivity
specificity_40 <- confusion_matrix_40[1,1] / sum(confusion_matrix_40[1,]) # Specificity

# Output the performance metrics
results <- data.frame(
  k = c(10, 20, 40),
  fp_rate = c(fp_rate_10, fp_rate_20, fp_rate_40),
  fn_rate = c(fn_rate_10, fn_rate_20, fn_rate_40),
  sensitivity = c(sensitivity_10, sensitivity_20, sensitivity_40),
  specificity = c(specificity_10, specificity_20, specificity_40)
)

print(results)

predictors_knn <- as.data.frame(as.matrix(developer_dummies))

## Plot the three KNN models
par(mfrow = c(1, 3))
# Plot for 10/20/40-nearest neighbors
plot(game[train, 'Rating'], predictors_knn[train, 2], col = y[train], cex = 0.8, pch = 18, xlab = "Rating", ylab = "log_activePlayers", main = "10-nearest neighbors")
points(game[-train, 'Rating'], predictors_knn[-train, 2], pch = 21, col = 1, cex = 1.25)
points(game[-train, 'Rating'], predictors_knn[-train, 2], bg = nearest10, pch = 21, col = grey(0.9), cex = 1.25)

plot(game[train, 'Rating'], predictors_knn[train, 2], col = y[train], cex = 0.8, pch = 18, xlab = "Rating", ylab = "log_activePlayers", main = "20-nearest neighbors")
points(game[-train, 'Rating'], predictors_knn[-train, 2], pch = 21, col = 1, cex = 1.25)
points(game[-train, 'Rating'], predictors_knn[-train, 2], bg = nearest20, pch = 21, col = grey(0.9), cex = 1.25)

plot(game[train, 'Rating'], predictors_knn[train, 2], col = y[train], cex = 0.8, pch = 18, xlab = "Rating", ylab = "log_activePlayers", main = "40-nearest neighbors")
points(game[-train, 'Rating'], predictors_knn[-train, 2], pch = 21, col = 1, cex = 1.25) 
points(game[-train, 'Rating'], predictors_knn[-train, 2], bg = nearest40, pch = 21, col = grey(0.9), cex = 1.25)

# Add legend
legend("topright", legend = levels(y), fill = 1:2, bty = "n", cex = 0.75)

```
```{r}
# Plot the three KNN models
par(mfrow = c(1, 3))

# Define colors for plot
colors <- c("black", "red")

# Plot for 10-nearest neighbors
plot(game$Rating[train], log(game$activePlayers[train]), col = colors[as.numeric(y[train])], cex = 0.8, pch = 18, xlab = "Rating", ylab = "log_activePlayers", main = "10-nearest neighbors")
points(game$Rating[-train], log(game$activePlayers[-train]), pch = 21, col = colors[as.numeric(nearest10)], cex = 1.25)

# Plot for 20-nearest neighbors
plot(game$Rating[train], log(game$activePlayers[train]), col = colors[as.numeric(y[train])], cex = 0.8, pch = 18, xlab = "Rating", ylab = "log_activePlayers", main = "20-nearest neighbors")
points(game$Rating[-train], log(game$activePlayers[-train]), pch = 21, col = colors[as.numeric(nearest20)], cex = 1.25)

# Plot for 40-nearest neighbors
plot(game$Rating[train], log(game$activePlayers[train]), col = colors[as.numeric(y[train])], cex = 0.8, pch = 18, xlab = "Rating", ylab = "log_activePlayers", main = "40-nearest neighbors")
points(game$Rating[-train], log(game$activePlayers[-train]), pch = 21, col = colors[as.numeric(nearest40)], cex = 1.25)

# Add legend
legend("topright", legend = c("0", "1"), fill = colors, bty = "n", cex = 0.75)
```


```{r}
set.seed(1234)
nearest <- class::knn(train=predictors_knn[train,], test=predictors_knn[-train,], cl=y[train], prob=TRUE, k=floor(40))  # sqrt(1735)
attr<-attributes(nearest)
t1 <- table(y[-train], nearest)
t1

# Calculate relevant rates
t1[1,2]/sum(t1[,2]) # FALSE POSITIVE RATE:   
t1[2,1]/sum(t1[,1]) # FALSE NEGATIVE RATE: 
t1[2,2]/sum(t1[2,]) # SENSITIVITY: 
t1[1,1]/sum(t1[1,]) # SPECIFICITY: 

source("roc.R")
roc(p= attr$prob, y=y[-train], bty="n")
title("ROC Curve after KNN Analysis, K=40")
```
```{r}
## 
probabilities <- attr$prob

# Combine the test set indices with their probabilities
test_indices <- (1:nrow(game))[-train]
prob_data <- data.frame(index = test_indices, probability = probabilities, nearest = nearest)

# Sort by probability to get top 5 games
top_5k <- prob_data %>%
  arrange(desc(probability)) %>%
  head(26)

top_5k

# Get the details of the top 5 games
top_5k_games <- game[top_5k$index, ]
top_5k_games <- top_5k_games[top_5k_games$activePlayers_dummy == 1, ]

print(top_5k_games)

# Function to get top three most common values
get_top_three <- function(x) {
  as.data.frame(sort(table(x), decreasing = TRUE)[1:10])}

# Analyze the characteristics of the top 5 games
characteristics <- top_5k_games %>%
  summarise(
    average_rating = mean(Rating),
    average_year = mean(year_adj)
  )

# Get top three genres, publishers, developers, and consoles
top_three_genres <- get_top_three(top_5k_games$genre)
top_three_publishers <- get_top_three(top_5k_games$publisher)
top_three_developers <- get_top_three(top_5k_games$developer)
top_three_consoles <- get_top_three(top_5k_games$console)

# Print results
print(characteristics)
print("Top Three Genres:")
print(top_three_genres)
print("Top Three Publishers:")
print(top_three_publishers)
print("Top Three Developers:")
print(top_three_developers)
print("Top Three Consoles:")
print(top_three_consoles)

```

## Multinomial Logistic Regression

```{r}
# Ensure response is a factor if it's multinomial
# Split data for cross-validation
set.seed(123)
quantiles <- quantile(game$activePlayers, probs=c(0, 0.5, 1), na.rm=TRUE)
y <- cut(game$activePlayers, breaks=quantiles, include.lowest=TRUE, labels=c(0, 1))
train_indices <- createDataPartition(y, p = 0.5, list = TRUE)
train_data <- predictors_knn[train_indices[[1]], ]
train_response <- y[train_indices[[1]]]
test_data <- predictors_knn[-train_indices[[1]], ]
test_response <- y[-train_indices[[1]]]
```


```{r}
set.seed(1234)

# Fit the glmnet model
cv_fit <- cv.glmnet(train_data, train_response, family = "multinomial")
plot(cv_fit, main="Cross-Validation")
par(mfrow=c(1,2))
plot(cv_fit$glmnet)

best_coefs <- coef(cv_fit, s = "lambda.min")
coefs_matrix <- as.matrix(best_coefs)
print(coefs_matrix)

# Predict and evaluate model
predictions <- predict(cv_fit, newx = test_data, s = "lambda.min", type = "response")
predicted_classes <- apply(predictions, 1, which.max)
accuracy <- mean(predicted_classes == test_response)
print(paste("Accuracy: ", accuracy))

# Plotting class probabilities
prob_matrix <- predict(cv_fit, newx = test_data, type = "response", s = "lambda.min")
boxplot(prob_matrix ~ test_response, col = "orange", varwidth = TRUE, main="Fit Plot of Test Response")

```

```{r}
# Split data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(y, p = 0.8, list = FALSE)
x_train <- predictors_knn[trainIndex,]
y_train <- y[trainIndex]
x_test <- predictors_knn[-trainIndex,]
y_test <- y[-trainIndex]

# KNN model
k <- 5  # Number of neighbors
knn_pred <- knn(train = x_train, test = x_test, cl = y_train, k = k)

# Check accuracy
table(Predicted = knn_pred, Actual = y_test)

# In-sample predictions for performance evaluation
knn_pred_train <- knn(train = x_train, test = x_train, cl = y_train, k = k)
in_sample_accuracy <- sum(knn_pred_train == y_train) / length(y_train)

library(class)
library(kknn)
# Using kknn for regression
knn_model <- kknn(activePlayers_dummy ~ ., train = x_train, test = x_test, y = y_train, k = 5)

# Extract predictions
knn_predictions <- fitted(knn_model)

# Calculate R-squared for the test set
ss_res <- sum((y_test - knn_predictions)^2)
ss_tot <- sum((y_test - mean(y_test))^2)
r_squared_out_sample <- 1 - ss_res / ss_tot

print(paste("Out-of-sample R^2:", r_squared_out_sample))



knn_pred <- class::knn.reg(train = x_train, test = x_test, y = y_train, k = k)
residuals <- y_test - knn_pred_train$pred

# Sum of Squares of residuals
ss_res <- sum(residuals^2)

# Total Sum of Squares
ss_tot <- sum((y_test - mean(y_test))^2)

# R-squared
r_squared <- 1 - (ss_res / ss_tot)

print(paste("Out-of-sample R^2:", r_squared))


# Out-of-sample accuracy
out_of_sample_accuracy <- sum(knn_pred == y_test) / length(y_test)

# Print accuracies
print(paste("In-sample Accuracy:", in_sample_accuracy))
print(paste("Out-of-sample Accuracy:", out_of_sample_accuracy))

k_fold_results <- train(predictors_knn , game$log_activePlayers, method = "knn", trControl = trainControl(method = "cv", number = 10), tuneGrid = expand.grid(k = c(1, 5, 10, 15, 20, 30)))

print(k_fold_results)

```

```{r}
# Run lasso Players_dummy on genre, summary words, and year
lasso4 <- gamlr(x_genre_sum_yr, game$Players_dummy, standardize=TRUE, family = "binomial", lambda.min.ratio=1e-3)
plot(lasso4)

# in-sample R2
1- lasso4$deviance[which.min(AICc(lasso4))]/lasso4$deviance[1]

# Check coefficients
lasso4coef <- coef(lasso4, select=which.min(AICc(lasso4)))
sum(lasso4coef!=0)

lasso4coef <- as.data.frame(as.matrix(lasso4coef))
colnames(lasso4coef) <- c("Coefficient")
lasso4coef$Feature <- rownames(lasso4coef)
rownames(lasso4coef) <- NULL
lasso4coef_sig <- lasso4coef %>% filter(Coefficient != 0)%>% arrange(desc(Coefficient))
lasso4coef_sig
```

```{r}
# Run CV lasso Players_dummy on genre, summary words, and year
set.seed(1234)
lasso4.cv <- cv.gamlr(x_genre_sum_yr, game$Players_dummy, lambda.min.ratio = 1e-3, family = "binomial", verb = TRUE)

# Plot the cross-validation results
plot(lasso4.cv)
abline(v=log(lasso4$lambda[which.min(AICc(lasso4))]))

# Display the coefficients of the Lasso model at the optimal lambda
coef_min <- coef(lasso4.cv, s = "min")
coef_1se <- coef(lasso4.cv, s = "1se")

Betas <- drop(coef_min) # AICc default selection: Betas <- drop(coef(lasso_sum))
length(coef_min)# intercept + 17 genres + 19 year + 1203 words

sum(coef_min!=0) # 1203 predictive words

# Choose 10 most positive review words in this model
o<-order(coef_min,decreasing=TRUE)  # [38:1240]
kable(Betas[o[1:10]])
o<-order(coef_min,decreasing=FALSE)  # [38:1240]
kable(Betas[o[1:15]])
```


```{r}
# Prepare data for regression
# game$log_Plays <- log(game$Plays + 1) # Add 1 to avoid log(0)

# Linear Regression
lm_model <- lm(log_activePlayers ~ genre + year + publisher + atvi_indi, data = game)
summary(lm_model)
```


```{r}
# Lasso Regression
x <- model.matrix(log_activePlayers ~ genre + year + publisher + atvi_indi + Rating + Number.of.Reviews, data = game)[, -1]

lasso_model <- cv.glmnet(x, game$log_activePlayers, alpha = 1)
plot(lasso_model)

# Coefficients of the Lasso model at the optimal lambda
lasso_coefs <- coef(lasso_model, s = "lambda.min")
print(lasso_coefs)

```

```{r}
# Decision Trees and Pruned Trees
# Decision Tree
tree_model <- rpart(log_activePlayers ~ x_tree, data = x_tree, method = "anova")
rpart.plot(tree_model)

# Prune the tree
pruned_tree <- prune(tree_model, cp = tree_model$cptable[which.min(tree_model$cptable[,"xerror"]),"CP"])
rpart.plot(pruned_tree)
```

```{r}
set.seed(123)
rf_model <- randomForest(log_Plays ~ genre + year, data = game, importance = TRUE, ntree = 500)
print(rf_model)

# Plot the importance of variables
varImpPlot(rf_model)
```


```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(tidyr)
library(RColorBrewer)
library(wordcloud)
library(tm)
library(sentimentr)
```


```{r}
# Descriptive statistics
summary(game$Rating)
summary(game$Total.Sales)

# Histogram of Ratings
p1 <- ggplot(game, aes(x = Rating)) +
  geom_histogram(binwidth = 0.1, fill = "blue", color = "black") +
  theme_minimal() +
  ggtitle("Distribution of Game Ratings")
print(p1)

game <- game %>%
  mutate(total_sales = ifelse(is.na(total_sales), 0, total_sales))

# Sales by Genre - Bar Chart
game %>%
  group_by(genre) %>%
  summarize(TotalSales = sum(total_sales, na.rm = TRUE)) %>%
  ggplot(aes(x = reorder(genre, -TotalSales), y = TotalSales, fill = genre)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Genre", y = "Total Sales", title = "Total Sales by Genre")

# Scatter Plot of Sales vs. Ratings
ggplot(game, aes(x = Rating, y = total_sales)) +
  geom_point(aes(color = genre), alpha = 0.6) +
  geom_smooth(method = lm) +
  theme_light() +
  labs(x = "Rating", y = "Total Sales", title = "Sales vs. Rating by Genre")
```


```{r}
varImpPlot(rf,  type=1, pch=21, bg="navy", main='RF variable importance')
```


```{r}
# Word Cloud of Game Reviews
text_corpus <- Corpus(VectorSource(game$Reviews))
text_corpus <- tm_map(text_corpus, content_transformer(tolower))
text_corpus <- tm_map(text_corpus, removePunctuation)
text_corpus <- tm_map(text_corpus, removeWords, stopwords("en"))
wordcloud(text_corpus, max.words = 100, random.order = FALSE, colors = brewer.pal(8, "Dark2"))

# Word Cloud of Game Summary
text_corpus <- Corpus(VectorSource(game$Summary))
text_corpus <- tm_map(text_corpus, content_transformer(tolower))
text_corpus <- tm_map(text_corpus, removePunctuation)
text_corpus <- tm_map(text_corpus, removeWords, stopwords("en"))
wordcloud(text_corpus, max.words = 100, random.order = FALSE, colors = brewer.pal(8, "Dark2"))

# Example of sentiment analysis
sentiments <- sentiment_by(game$Reviews)
mean(sentiments$sentiment)

```

